{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e0efa-fc79-4d45-9735-b514c2a2c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install oracledb openai google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb4c59-869f-4831-897c-56512cf80163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CONFIGURATION CELL - SET ALL VARIABLES HERE\n",
    "# ====================================================================\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# =========================\n",
    "# USER CONFIGURATION\n",
    "# =========================\n",
    "\n",
    "# API Configuration\n",
    "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google Gemini API key: \")\n",
    "\n",
    "# Package Configuration - ADD YOUR PACKAGES HERE\n",
    "PACKAGES_TO_PROCESS = [\n",
    "    \"GIN_STP_PKG\",\n",
    "    \"GIN_ACCOUNTS_PKG\",\n",
    "]\n",
    "\n",
    "# Database Configurations - MODIFY THESE FOR YOUR DATABASES\n",
    "DATABASE_CONFIGS = {\n",
    "    \"DATABASE_A\": { \n",
    "        \"host\": \"10.176.18.91\",\n",
    "        \"port\": 1522,\n",
    "        \"service_name\": \"HERITAGE19C\",\n",
    "        \"username\": \"TQ_GIS\",\n",
    "        \"password\": \"TQ_GIS\"\n",
    "    },\n",
    "    \"DATABASE_B\": { \n",
    "        \"host\": \"10.176.18.110\",\n",
    "        \"port\": 1521,\n",
    "        \"service_name\": \"NEW_GEMINIA\",\n",
    "        \"username\": \"TQ_GIS\",\n",
    "        \"password\": \"TQ_GIS\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Output Directories\n",
    "BASE_DIRECTORY = 'packages'\n",
    "DIFFS_DIRECTORY = 'diffs'\n",
    "REPORTS_DIRECTORY = 'reports'\n",
    "OUTPUT_DIRECTORY = 'output'\n",
    "CONSOLIDATED_DIRECTORY = 'consolidated_packages'\n",
    "\n",
    "# Logging Configuration - ERROR will only show errors, INFO shows all operations\n",
    "LOG_LEVEL = 'ERROR'  # Change to 'INFO' if you want detailed logging\n",
    "\n",
    "# ====================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# ====================================================================\n",
    "\n",
    "import oracledb as cx_Oracle\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import difflib\n",
    "import shutil\n",
    "import google.generativeai as genai\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=getattr(logging, LOG_LEVEL.upper(), logging.ERROR), \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Configure Google Gemini\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"Google Gemini API key not found. Please set the 'GOOGLE_API_KEY' environment variable.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# ====================================================================\n",
    "# CORE FUNCTIONS\n",
    "# ====================================================================\n",
    "\n",
    "def clean_sql_content(content: str) -> str:\n",
    "    \"\"\"Clean SQL content by removing code blocks and extra whitespace\"\"\"\n",
    "    content = re.sub(r'^```sql\\s*\\n?', '', content, flags=re.IGNORECASE)\n",
    "    content = re.sub(r'\\n?\\s*```\\s*$', '', content)\n",
    "    content = re.sub(r'\\n?\\s*FINAL_MERGING_DONE\\s*$', '', content, flags=re.IGNORECASE)\n",
    "    return content.strip()\n",
    "\n",
    "def get_package_source(db_params, package_name, object_type='PACKAGE BODY'):\n",
    "    \"\"\"Retrieve package source code from database\"\"\"\n",
    "    logging.info(f\"Connecting to database {db_params['service_name']} to retrieve {object_type} '{package_name}'.\")\n",
    "    \n",
    "    try:\n",
    "        dsn_tns = cx_Oracle.makedsn(\n",
    "            db_params['host'],\n",
    "            db_params['port'],\n",
    "            service_name=db_params['service_name']\n",
    "        )\n",
    "        conn = cx_Oracle.connect(\n",
    "            user=db_params['username'],\n",
    "            password=db_params['password'],\n",
    "            dsn=dsn_tns\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT text\n",
    "        FROM all_source\n",
    "        WHERE name = '{package_name.upper()}'\n",
    "        AND type = '{object_type.upper()}'\n",
    "        ORDER BY line\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(query)\n",
    "        source_lines = [row[0] for row in cursor.fetchall()]\n",
    "        source = ''.join(source_lines)\n",
    "        \n",
    "        logging.info(f\"Retrieved {len(source)} characters of source code from {db_params['service_name']}.\")\n",
    "        \n",
    "    except cx_Oracle.DatabaseError as e:\n",
    "        logging.error(f\"Database connection failed: {e}\")\n",
    "        source = \"\"\n",
    "    finally:\n",
    "        try:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return source\n",
    "\n",
    "def parse_package_components(source_code):\n",
    "    \"\"\"Parse package components (procedures, functions, cursors, types, variables)\"\"\"\n",
    "    logging.info(\"Parsing package components.\")\n",
    "    \n",
    "    components = {\n",
    "        'procedures': {},\n",
    "        'functions': {},\n",
    "        'cursors': {},\n",
    "        'types': {},\n",
    "        'variables': {},\n",
    "    }\n",
    "    \n",
    "    # Patterns\n",
    "    proc_pattern = re.compile(\n",
    "        r\"\"\"\n",
    "        PROCEDURE\\s+([\\w$]+)\\s*\n",
    "        \\(.*?\\)\\s*\n",
    "        (.*?)\n",
    "        (?=PROCEDURE|FUNCTION|\\Z)\n",
    "        \"\"\",\n",
    "        re.IGNORECASE | re.DOTALL | re.VERBOSE\n",
    "    )\n",
    "    func_pattern = re.compile(\n",
    "        r\"\"\"\n",
    "        FUNCTION\\s+([\\w$]+)\\s*\n",
    "        \\(.*?\\)\\s*\n",
    "        (.*?)\n",
    "        (?=PROCEDURE|FUNCTION|\\Z)\n",
    "        \"\"\",\n",
    "        re.IGNORECASE | re.DOTALL | re.VERBOSE\n",
    "    )\n",
    "    \n",
    "    # Procedures\n",
    "    for match in proc_pattern.finditer(source_code):\n",
    "        name = match.group(1)\n",
    "        components['procedures'][name] = match.group(0).strip()\n",
    "    # Functions\n",
    "    for match in func_pattern.finditer(source_code):\n",
    "        name = match.group(1)\n",
    "        components['functions'][name] = match.group(0).strip()\n",
    "    \n",
    "    # Declarations\n",
    "    decl_match = re.search(r'(IS|AS)\\s+(.*?)\\s+BEGIN', source_code, re.IGNORECASE | re.DOTALL)\n",
    "    if decl_match:\n",
    "        decl = decl_match.group(2)\n",
    "        cursor_pattern = re.compile(r'CURSOR\\s+([\\w$]+)\\s*(IS|AS)\\s+(.*?);', re.IGNORECASE | re.DOTALL)\n",
    "        type_pattern = re.compile(r'TYPE\\s+([\\w$]+)\\s+(IS|AS)\\s+(.*?);', re.IGNORECASE | re.DOTALL)\n",
    "        variable_pattern = re.compile(r'(\\w+)\\s+(CONSTANT\\s+)?[\\w%\\.]+(\\([\\d\\s,]*\\))?\\s*(NOT\\s+NULL)?*\\s*(:=\\s*.*|)\\s*;', re.IGNORECASE | re.DOTALL)\n",
    "        for m in cursor_pattern.finditer(decl):\n",
    "            components['cursors'][m.group(1)] = m.group(0).strip()\n",
    "        for m in type_pattern.finditer(decl):\n",
    "            components['types'][m.group(1)] = m.group(0).strip()\n",
    "        for m in variable_pattern.finditer(decl):\n",
    "            components['variables'][m.group(1)] = m.group(0).strip()\n",
    "    return components\n",
    "\n",
    "def save_components_to_disk(components, package_name, base_directory=BASE_DIRECTORY):\n",
    "    \"\"\"Save package components to disk\"\"\"\n",
    "    package_dir = os.path.join(base_directory, package_name)\n",
    "    os.makedirs(package_dir, exist_ok=True)\n",
    "    for comp_type, comp_dict in components.items():\n",
    "        type_dir = os.path.join(package_dir, comp_type)\n",
    "        os.makedirs(type_dir, exist_ok=True)\n",
    "        for name, definition in comp_dict.items():\n",
    "            safe_name = ''.join(c if c.isalnum() or c in ' _-' else '_' for c in name)\n",
    "            with open(os.path.join(type_dir, f\"{safe_name}.sql\"), 'w', encoding='utf-8') as f:\n",
    "                f.write(definition)\n",
    "\n",
    "def save_components_as_json(components, package_name, base_directory=BASE_DIRECTORY):\n",
    "    \"\"\"Save components as JSON\"\"\"\n",
    "    package_dir = os.path.join(base_directory, package_name)\n",
    "    os.makedirs(package_dir, exist_ok=True)\n",
    "    with open(os.path.join(package_dir, f\"{package_name}_components.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(components, f, indent=4)\n",
    "\n",
    "def compare_components(components1, components2, package_name):\n",
    "    \"\"\"Compare components and generate diffs\"\"\"\n",
    "    differences = {}\n",
    "    diffs_output_dir = os.path.join(DIFFS_DIRECTORY, package_name)\n",
    "    os.makedirs(diffs_output_dir, exist_ok=True)\n",
    "    \n",
    "    for comp_type in components1.keys():\n",
    "        set1, set2 = set(components1[comp_type]), set(components2[comp_type])\n",
    "        added, removed, modified = set2 - set1, set1 - set2, set()\n",
    "        for common in set1 & set2:\n",
    "            content1 = components1[comp_type][common].strip().splitlines()\n",
    "            content2 = components2[comp_type][common].strip().splitlines()\n",
    "            if content1 != content2:\n",
    "                modified.add(common)\n",
    "                diff = difflib.unified_diff(content1, content2,\n",
    "                                            fromfile=f'{package_name}_DBA_{comp_type}_{common}.sql',\n",
    "                                            tofile=f'{package_name}_DBB_{comp_type}_{common}.sql',\n",
    "                                            lineterm='')\n",
    "                with open(os.path.join(diffs_output_dir, f'{comp_type}_{common}_diff.txt'), 'w', encoding='utf-8') as f:\n",
    "                    f.write('\\n'.join(diff))\n",
    "        differences[comp_type] = {'added': list(added), 'removed': list(removed), 'modified': list(modified)}\n",
    "    return differences\n",
    "\n",
    "def merge_database_package_procedures_with_history(differences_file_path, heritage_package_procedure, geminia_package_procedure, output_package_path):\n",
    "    \"\"\"Merge procedures using Gemini AI\"\"\"\n",
    "    generation_config = {\"temperature\": 0.2, \"top_p\": 0.95, \"max_output_tokens\": 8192, \"response_mime_type\": \"text/plain\"}\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-1.5-flash\",\n",
    "        generation_config=generation_config,\n",
    "        system_instruction=\"You are a senior PL/SQL developer. Merge procedures and functions. End with FINAL_MERGING_DONE.\",\n",
    "    )\n",
    "    with open(differences_file_path) as f: diff_content = f.read()\n",
    "    with open(heritage_package_procedure) as f: heritage_content = f.read()\n",
    "    with open(geminia_package_procedure) as f: geminia_content = f.read()\n",
    "    base_prompt = f\"\"\"\n",
    "    Merge two PL/SQL procedures.\n",
    "    Diff (context only): {diff_content}\n",
    "    Database A: {heritage_content}\n",
    "    Database B: {geminia_content}\n",
    "    Write FINAL_MERGING_DONE at the end.\n",
    "    \"\"\"\n",
    "    merged_chunks, history = [], [{\"role\": \"user\", \"parts\": [base_prompt]}]\n",
    "    chat_session = model.start_chat(history=history)\n",
    "    while True:\n",
    "        response = chat_session.send_message(\"continue\")\n",
    "        chunk = response.text.strip()\n",
    "        merged_chunks.append(chunk)\n",
    "        history.append({\"role\": \"model\", \"parts\": [chunk]})\n",
    "        if \"FINAL_MERGING_DONE\" in chunk.upper():\n",
    "            merged_chunks[-1] = merged_chunks[-1].replace(\"FINAL_MERGING_DONE\", \"\").strip()\n",
    "            break\n",
    "        else:\n",
    "            history.append({\"role\": \"user\", \"parts\": [\"Continue.\"]})\n",
    "            chat_session = model.start_chat(history=history)\n",
    "    merged_content = clean_sql_content(\"\\n\\n\".join(merged_chunks))\n",
    "    os.makedirs(os.path.dirname(output_package_path) or \".\", exist_ok=True)\n",
    "    with open(output_package_path, \"w\") as f: f.write(merged_content)\n",
    "    return output_package_path\n",
    "\n",
    "def merge_all_database_procedures_and_functions(package_name, differences_file, heritage_path, geminia_path, output_dir):\n",
    "    \"\"\"Merge all procedures/functions or copy if only in DB_B\"\"\"\n",
    "    with open(differences_file) as f: differences = json.load(f)\n",
    "    package_output_dir = os.path.join(output_dir, package_name)\n",
    "    os.makedirs(package_output_dir, exist_ok=True)\n",
    "    merged_files = []\n",
    "    for comp_type, diff_data in differences.items():\n",
    "        if comp_type not in ['procedures', 'functions']:\n",
    "            continue\n",
    "        # modified → Gemini\n",
    "        for name in diff_data.get('modified', []):\n",
    "            diff_path = os.path.join(DIFFS_DIRECTORY, package_name, f\"{comp_type}_{name}_diff.txt\")\n",
    "            db_a_file = os.path.join(heritage_path, comp_type, f\"{name}.sql\")\n",
    "            db_b_file = os.path.join(geminia_path, comp_type, f\"{name}.sql\")\n",
    "            out_file = os.path.join(package_output_dir, f\"merged_{name}.sql\")\n",
    "            if all(os.path.exists(p) for p in [diff_path, db_a_file, db_b_file]):\n",
    "                merged_files.append(merge_database_package_procedures_with_history(diff_path, db_a_file, db_b_file, out_file))\n",
    "        # added → copy DB_B\n",
    "        for name in diff_data.get('added', []):\n",
    "            db_b_file = os.path.join(geminia_path, comp_type, f\"{name}.sql\")\n",
    "            out_file = os.path.join(package_output_dir, f\"merged_{name}.sql\")\n",
    "            if os.path.exists(db_b_file):\n",
    "                shutil.copyfile(db_b_file, out_file)\n",
    "                with open(out_file, \"r+\", encoding=\"utf-8\") as f:\n",
    "                    cleaned = clean_sql_content(f.read())\n",
    "                    f.seek(0); f.write(cleaned); f.truncate()\n",
    "                merged_files.append(out_file)\n",
    "    return merged_files\n",
    "\n",
    "def consolidate_merged_procedures_and_functions(package_name, merged_files_list, output_dir):\n",
    "    \"\"\"Consolidate all merged files into a single package\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    consolidated_path = os.path.join(output_dir, f\"{package_name}_MERGED.sql\")\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    consolidated = f\"\"\"-- =====================================================\n",
    "-- Package: {package_name}\n",
    "-- Generated: {timestamp}\n",
    "-- Description: Merged procedures and functions from Database A and B\n",
    "-- =====================================================\n",
    "\n",
    "CREATE OR REPLACE PACKAGE BODY {package_name} AS\n",
    "\n",
    "\"\"\"\n",
    "    for file_path in sorted(merged_files_list):\n",
    "        if not os.path.exists(file_path): continue\n",
    "        with open(file_path) as f: cleaned = clean_sql_content(f.read())\n",
    "        if cleaned.strip():\n",
    "            name = os.path.basename(file_path).replace('merged_', '').replace('.sql', '')\n",
    "            consolidated += f\"\\n  -- {'-'*50}\\n  -- {name.upper()}\\n  -- {'-'*50}\\n\\n\"\n",
    "            consolidated += '\\n'.join(['  ' + line if line.strip() else line for line in cleaned.splitlines()]) + \"\\n\\n\"\n",
    "    consolidated += \"END;\\n/\"\n",
    "    with open(consolidated_path, \"w\") as f: f.write(consolidated)\n",
    "    return consolidated_path\n",
    "\n",
    "def process_single_package(package_name):\n",
    "    \"\"\"Process a single package through the entire workflow\"\"\"\n",
    "    print(f\"\\n=== PROCESSING PACKAGE: {package_name} ===\")\n",
    "    src_a = get_package_source(DATABASE_CONFIGS['DATABASE_A'], package_name)\n",
    "    src_b = get_package_source(DATABASE_CONFIGS['DATABASE_B'], package_name)\n",
    "    if not src_a or not src_b:\n",
    "        print(f\"Failed to retrieve package {package_name}\")\n",
    "        return False\n",
    "    comps_a, comps_b = parse_package_components(src_a), parse_package_components(src_b)\n",
    "    save_components_to_disk(comps_a, package_name + \"_DATABASE_A_BODY\")\n",
    "    save_components_to_disk(comps_b, package_name + \"_DATABASE_B_BODY\")\n",
    "    save_components_as_json(comps_a, package_name + \"_DATABASE_A_BODY\")\n",
    "    save_components_as_json(comps_b, package_name + \"_DATABASE_B_BODY\")\n",
    "    differences = compare_components(comps_a, comps_b, package_name)\n",
    "    diff_file = os.path.join(DIFFS_DIRECTORY, package_name, \"differences.json\")\n",
    "    with open(diff_file, \"w\") as f: json.dump(differences, f, indent=4)\n",
    "    merged_files = merge_all_database_procedures_and_functions(\n",
    "        package_name, diff_file,\n",
    "        f\"packages/{package_name}_DATABASE_A_BODY\",\n",
    "        f\"packages/{package_name}_DATABASE_B_BODY\",\n",
    "        OUTPUT_DIRECTORY\n",
    "    )\n",
    "    if merged_files:\n",
    "        consolidated = consolidate_merged_procedures_and_functions(package_name, merged_files, CONSOLIDATED_DIRECTORY)\n",
    "        print(f\"Consolidated package created: {consolidated}\")\n",
    "    else:\n",
    "        print(f\"No differences to merge for {package_name}\")\n",
    "    return True\n",
    "\n",
    "def run_package_comparison():\n",
    "    \"\"\"Main entrypoint to process all packages\"\"\"\n",
    "    print(\"Starting Package Comparison and Merging Tool\")\n",
    "    for pkg in PACKAGES_TO_PROCESS:\n",
    "        process_single_package(pkg)\n",
    "    print(\"Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc59d535-7101-4ea5-9ffa-45c784c48b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# EXECUTION CELL - RUN THIS TO START THE PROCESS\n",
    "# ====================================================================\n",
    "\n",
    "run_package_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ef1ad-b122-4ba6-8c5a-6c13a4070b47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
