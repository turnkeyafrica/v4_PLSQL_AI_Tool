{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d075a5-6a53-4e53-af6b-30674f758910",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sqlalchemy pandas oracledb openai google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad2dd9a-c66d-410b-a1fc-b7a09ce81c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 10:29:34,038 - INFO - Connecting to database: HERITAGE\n",
      "2024-12-03 10:29:34,044 - INFO - Engine created for database with service name 'HERITAGEINT'.\n",
      "2024-12-03 10:29:36,080 - INFO - Extracting source for package: GIN_STP_PKG\n",
      "2024-12-03 10:32:20,783 - INFO - Extracting metadata for package: GIN_STP_PKG\n",
      "2024-12-03 10:32:21,570 - INFO - Disposed engine for database: HERITAGE\n",
      "2024-12-03 10:32:21,572 - INFO - Connecting to database: NEW_GEMINIA\n",
      "2024-12-03 10:32:21,576 - INFO - Engine created for database with service name 'NEW_GEMINIA'.\n",
      "2024-12-03 10:32:23,439 - INFO - Extracting source for package: GIN_STP_PKG\n",
      "2024-12-03 10:34:24,758 - INFO - Extracting metadata for package: GIN_STP_PKG\n",
      "2024-12-03 10:34:25,474 - INFO - Disposed engine for database: NEW_GEMINIA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed. Archive `package_extracts.zip` is ready.\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy as sa\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "import shutil\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, force=True, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# create a database connection\n",
    "def create_database_connection(host, port, service_name, username, password):\n",
    "    \"\"\"\n",
    "    Create a connection to an Oracle database using SQLAlchemy.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection_url = f\"oracle+oracledb://{username}:{password}@{host}:{port}/?service_name={service_name}\"\n",
    "        engine = sa.create_engine(connection_url)\n",
    "        logging.info(f\"Engine created for database with service name '{service_name}'.\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating engine for service '{service_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "def extract_package_source(engine, package_name: str) -> Dict[str, str]:\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            logging.info(f\"Extracting source for package: {package_name}\")\n",
    "            query = \"\"\"\n",
    "            SELECT text, line, type\n",
    "            FROM all_source\n",
    "            WHERE name = :package_name\n",
    "            AND type IN ('PACKAGE', 'PACKAGE BODY')\n",
    "            ORDER BY type, line\n",
    "            \"\"\"\n",
    "            result = connection.execute(sa.text(query), {\"package_name\": package_name})\n",
    "            package_spec, package_body = [], []\n",
    "            for row in result:\n",
    "                if row.type == 'PACKAGE':\n",
    "                    package_spec.append(row.text)\n",
    "                elif row.type == 'PACKAGE BODY':\n",
    "                    package_body.append(row.text)\n",
    "            return {\n",
    "                \"package_spec\": \"\\n\".join(package_spec),\n",
    "                \"package_body\": \"\\n\".join(package_body),\n",
    "            }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting package {package_name}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def extract_package_metadata(engine, package_name: str) -> Dict[str, Optional[str]]:\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            logging.info(f\"Extracting metadata for package: {package_name}\")\n",
    "            query = \"\"\"\n",
    "            SELECT \n",
    "                last_ddl_time, \n",
    "                created,\n",
    "                status,\n",
    "                object_type\n",
    "            FROM all_objects\n",
    "            WHERE object_name = :package_name\n",
    "            AND object_type IN ('PACKAGE', 'PACKAGE BODY')\n",
    "            \"\"\"\n",
    "            result = connection.execute(sa.text(query), {\"package_name\": package_name}).fetchall()\n",
    "            if result:\n",
    "                return {\n",
    "                    \"last_modified\": result[0].last_ddl_time,\n",
    "                    \"created_date\": result[0].created,\n",
    "                    \"status\": result[0].status,\n",
    "                    \"object_type\": result[0].object_type,\n",
    "                }\n",
    "            else:\n",
    "                return {}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting metadata for package {package_name}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def extract_packages_from_databases(\n",
    "    database_credentials: Dict, \n",
    "    packages_to_extract: List[str], \n",
    "    output_directory: str = 'package_extracts'\n",
    ") -> Dict:\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    extraction_results = {}\n",
    "\n",
    "    for db_name, credentials in database_credentials.items():\n",
    "        logging.info(f\"Connecting to database: {db_name}\")\n",
    "        try:\n",
    "            engine = create_database_connection(\n",
    "                credentials[\"host\"],\n",
    "                credentials[\"port\"],\n",
    "                credentials[\"service_name\"],\n",
    "                credentials[\"username\"],\n",
    "                credentials[\"password\"]\n",
    "            )\n",
    "            db_results = {}\n",
    "\n",
    "            for package_name in packages_to_extract:\n",
    "                package_source = extract_package_source(engine, package_name)\n",
    "                package_metadata = extract_package_metadata(engine, package_name)\n",
    "                \n",
    "                if package_source or package_metadata:\n",
    "                    db_results[package_name] = {\n",
    "                        \"source\": package_source,\n",
    "                        \"metadata\": package_metadata,\n",
    "                    }\n",
    "                    \n",
    "                    # Prepare saving the files\n",
    "                    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                    package_dir = os.path.join(output_directory, db_name, package_name)\n",
    "                    os.makedirs(package_dir, exist_ok=True)\n",
    "\n",
    "                    # Save source code to files\n",
    "                    if package_source.get(\"package_spec\"):\n",
    "                        spec_path = os.path.join(package_dir, f\"{package_name}_spec_{timestamp}.sql\")\n",
    "                        with open(spec_path, \"w\") as f:\n",
    "                            f.write(package_source[\"package_spec\"])\n",
    "                    if package_source.get(\"package_body\"):\n",
    "                        body_path = os.path.join(package_dir, f\"{package_name}_body_{timestamp}.sql\")\n",
    "                        with open(body_path, \"w\") as f:\n",
    "                            f.write(package_source[\"package_body\"])\n",
    "\n",
    "            extraction_results[db_name] = db_results\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to process database {db_name}. Error: {e}\")\n",
    "        finally:\n",
    "            if 'engine' in locals() and engine:\n",
    "                engine.dispose()\n",
    "                logging.info(f\"Disposed engine for database: {db_name}\")\n",
    "\n",
    "    return extraction_results\n",
    "\n",
    "database_credentials = {\n",
    "    \"HERITAGE\": {\n",
    "        \"host\": \"10.176.18.91\",\n",
    "        \"port\": 1522,\n",
    "        \"service_name\": \"HERITAGEINT\",\n",
    "        \"username\": \"TQ_GIS\",\n",
    "        \"password\": \"TQ_GIS\"\n",
    "    },\n",
    "    \"NEW_GEMINIA\": {\n",
    "        \"host\": \"10.176.18.110\",\n",
    "        \"port\": 1523,\n",
    "        \"service_name\": \"NEW_GEMINIA\",\n",
    "        \"username\": \"TQ_GIS\",\n",
    "        \"password\": \"TQ_GIS\"\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "packages_to_extract = [\"GIN_STP_PKG\"]\n",
    "results = extract_packages_from_databases(database_credentials, packages_to_extract)\n",
    "\n",
    "output_zip = \"package_extracts.zip\"\n",
    "shutil.make_archive(\"package_extracts\", 'zip', \"package_extracts\")\n",
    "\n",
    "print(\"Extraction completed. Archive `package_extracts.zip` is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86970459-30cc-4051-a6a7-50e6a1150247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "from typing import Dict, List\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, force=True, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "def parse_package_source(package_source: str) -> Dict[str, str]:\n",
    "    parsed_units = {}\n",
    "    \n",
    "    try:\n",
    "        unit_pattern = re.compile(\n",
    "            r\"(?i)(PROCEDURE|FUNCTION)\\s+(\\w+)\\s*(\\(.*?\\))?\\s*(IS|AS)\",\n",
    "            re.DOTALL\n",
    "        )\n",
    "        matches = list(unit_pattern.finditer(package_source))\n",
    "        \n",
    "        for i, match in enumerate(matches):\n",
    "            unit_type = match.group(1)\n",
    "            unit_name = match.group(2)\n",
    "            if not unit_type or not unit_name:\n",
    "                logging.warning(\"Skipping invalid match during parsing.\")\n",
    "                continue\n",
    "\n",
    "            unit_type = unit_type.upper()\n",
    "            start_idx = match.start()      \n",
    "            end_idx = matches[i + 1].start() if i + 1 < len(matches) else len(package_source)\n",
    "            \n",
    "            # Extract the code block for this unit\n",
    "            unit_code = package_source[start_idx:end_idx].strip()\n",
    "            parsed_units[f\"{unit_type} {unit_name}\"] = unit_code\n",
    "            \n",
    "        return parsed_units\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing package source: {e}\")\n",
    "        return {}\n",
    "\n",
    "def parse_packages(results: Dict) -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Parses all extracted packages into their constituent units (procedures/functions).\n",
    "\n",
    "    Args:\n",
    "        results (Dict): Dictionary containing extracted package sources.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, str]]: A nested dictionary where each package maps to its units.\n",
    "    \"\"\"\n",
    "    parsed_results = {}\n",
    "\n",
    "    for db_name, packages in results.items():\n",
    "        logging.info(f\"Parsing packages for database: {db_name}\")\n",
    "        db_parsed = {}\n",
    "        \n",
    "        for package_name, package_data in packages.items():\n",
    "            package_source = package_data.get(\"source\", {}).get(\"package_spec\", \"\") + \"\\n\" + \\\n",
    "                             package_data.get(\"source\", {}).get(\"package_body\", \"\")\n",
    "            if package_source.strip():\n",
    "                db_parsed[package_name] = parse_package_source(package_source)\n",
    "            else:\n",
    "                logging.warning(f\"No source found for package: {package_name}\")\n",
    "        \n",
    "        parsed_results[db_name] = db_parsed\n",
    "    \n",
    "    return parsed_results\n",
    "\n",
    "# Parse the extracted packages\n",
    "parsed_packages = parse_packages(results)\n",
    "\n",
    "# output structure\n",
    "for db, packages in parsed_packages.items():\n",
    "    logging.info(f\"Database: {db}\")\n",
    "    for pkg, units in packages.items():\n",
    "        logging.info(f\"  Package: {pkg}\")\n",
    "        for unit_name, code in units.items():\n",
    "            logging.info(f\"    Unit: {unit_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0ea660-8f56-48b4-ba82-7f0c9fc7d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "client = openai.OpenAI(api_key='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')\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def normalize_code(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize code by removing comments and excess whitespace\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lines = code.splitlines()\n",
    "        cleaned_lines = [line.split('--', 1)[0].strip() for line in lines if line.strip()]\n",
    "        \n",
    "        normalized_code = '\\n'.join(cleaned_lines)\n",
    "        return normalized_code\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during normalization: {e}\")\n",
    "        logging.error(f\"Offending code:\\n{code}\")\n",
    "        return code\n",
    "\n",
    "def extract_json_from_text(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Attempt to extract a valid JSON structure from the text\n",
    "    \"\"\"\n",
    "    # First, try direct JSON parsing\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        json_match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                return json.loads(json_match.group(0))\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "    \n",
    "    # If all parsing fails, return a default structure\n",
    "    return {\n",
    "        'additions': [],\n",
    "        'deletions': [],\n",
    "        'modifications': [],\n",
    "        'missing_procedures': {\n",
    "            'first_db': [],\n",
    "            'second_db': []\n",
    "        },\n",
    "        'total_changes': {\n",
    "            'added_lines': 0,\n",
    "            'deleted_lines': 0,\n",
    "            'modified_lines': 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "def gpt4_mini_code_comparison(old_code: str, new_code: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        prompt = f\"\"\"\n",
    "                    Compare the following two blocks of code and identify the differences. Provide a structured response in JSON format with the following keys:\n",
    "                    - 'additions': List of lines added in the new code\n",
    "                    - 'deletions': List of lines removed from the old code\n",
    "                    - 'modifications': List of lines that have been modified\n",
    "                    - 'missing_procedures': Dictionary with 'first_db' and 'second_db' keys listing any missing procedures\n",
    "                    - 'total_changes': Dictionary with counts of added, deleted, and modified lines\n",
    "                    \n",
    "                    IMPORTANT: Ensure the response is a valid JSON. Do not include any explanatory text outside the JSON.\n",
    "                    \n",
    "                    Old Code:\n",
    "                    {old_code}\n",
    "                    \n",
    "                    New Code:\n",
    "                    {new_code}\n",
    "                    \"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a precise code comparison assistant. Always respond with a valid JSON structure.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=1000,\n",
    "            temperature=0.5,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "\n",
    "        gpt_report = response.choices[0].message.content.strip()\n",
    "\n",
    "        differences = extract_json_from_text(gpt_report)\n",
    "\n",
    "        return differences\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during GPT-4 comparison: {e}\")\n",
    "        return extract_json_from_text(\"{}\")\n",
    "\n",
    "def generate_diff_reports(parsed_packages: Dict[str, Dict[str, Dict[str, str]]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate difference reports for units across different database versions using GPT-4.\n",
    "    \n",
    "    Args:\n",
    "        parsed_packages (Dict): Dictionary containing parsed unit data for each version.\n",
    "    \n",
    "    Returns:\n",
    "        Dict: A comprehensive dictionary of difference reports.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f\"diff_reports_{timestamp}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    diff_reports = {}\n",
    "    db_names = list(parsed_packages.keys())\n",
    "    \n",
    "    if len(db_names) < 2:\n",
    "        logging.error(\"Need at least two databases for comparison\")\n",
    "        return {}\n",
    "    \n",
    "    first_db, second_db = db_names[0], db_names[1]\n",
    "    \n",
    "    for package_name in parsed_packages[first_db].keys():\n",
    "        if package_name not in parsed_packages[second_db]:\n",
    "            continue\n",
    "        \n",
    "        package_diff_reports = {}\n",
    "        first_package_units = parsed_packages[first_db][package_name]\n",
    "        second_package_units = parsed_packages[second_db][package_name]\n",
    "        \n",
    "        # Track missing procedures in each package\n",
    "        missing_in_first = set(second_package_units.keys()) - set(first_package_units.keys())\n",
    "        missing_in_second = set(first_package_units.keys()) - set(second_package_units.keys())\n",
    "        \n",
    "        # Report missing procedures\n",
    "        if missing_in_first:\n",
    "            logging.info(f\"Procedures in {second_db} but missing in {first_db}: {missing_in_first}\")\n",
    "        if missing_in_second:\n",
    "            logging.info(f\"Procedures in {first_db} but missing in {second_db}: {missing_in_second}\")\n",
    "        \n",
    "        for unit_name in set(first_package_units.keys()) & set(second_package_units.keys()):\n",
    "            try:\n",
    "                first_unit_code = first_package_units[unit_name]\n",
    "                second_unit_code = second_package_units[unit_name]\n",
    "                \n",
    "                differences = gpt4_mini_code_comparison(first_unit_code, second_unit_code)\n",
    "                \n",
    "                # Add missing procedures info to the differences if found\n",
    "                if missing_in_first:\n",
    "                    differences[\"missing_procedures\"][\"first_db\"] = list(missing_in_first)\n",
    "                if missing_in_second:\n",
    "                    differences[\"missing_procedures\"][\"second_db\"] = list(missing_in_second)\n",
    "\n",
    "                if differences:\n",
    "                    differences[\"source_database\"] = first_db\n",
    "                    differences[\"target_database\"] = second_db\n",
    "                    package_diff_reports[unit_name] = differences\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error comparing unit {unit_name}: {e}\")\n",
    "        \n",
    "        if package_diff_reports:\n",
    "            diff_reports[package_name] = package_diff_reports\n",
    "    \n",
    "    report_path = os.path.join(output_dir, \"code_differences_report.json\")\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(diff_reports, f, indent=2)\n",
    "    \n",
    "    return diff_reports\n",
    "\n",
    "diff_reports = generate_diff_reports(parsed_packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "888e131a-70a9-459c-a0c7-a0c864171d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report saved at: diff_reports_20241203_162053\\code_differences_report.json\n",
      "The diff_reports object is valid JSON.\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List, Union\n",
    "\n",
    "\n",
    "def ensure_json_serializable(obj):\n",
    "    \"\"\"Ensure the object is JSON-serializable.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: ensure_json_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [ensure_json_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    elif hasattr(obj, \"__dict__\"):\n",
    "        return ensure_json_serializable(obj.__dict__)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def normalize_code(code: str, as_list=False) -> Union[List[str], str]:\n",
    "    lines = [line.split(\"--\", 1)[0].strip() for line in code.splitlines() if line.strip()]\n",
    "    return lines if as_list else \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def compare_code(source_code: str, target_code: str) -> Dict[str, Any]:\n",
    "    \"\"\"Compare two blocks of code and identify differences.\"\"\"\n",
    "    old_lines = normalize_code(source_code, as_list=True)\n",
    "    new_lines = normalize_code(target_code, as_list=True)\n",
    "    matcher = difflib.SequenceMatcher(None, old_lines, new_lines)\n",
    "    additions, deletions, modifications = [], [], []\n",
    "\n",
    "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "        if tag == \"replace\":\n",
    "            modifications.extend(\n",
    "                [{\"source\": old_lines[i], \"target\": new_lines[j]} for i, j in zip(range(i1, i2), range(j1, j2))]\n",
    "            )\n",
    "        elif tag == \"delete\":\n",
    "            deletions.extend(old_lines[i1:i2])\n",
    "        elif tag == \"insert\":\n",
    "            additions.extend(new_lines[j1:j2])\n",
    "\n",
    "    return {\n",
    "        \"additions\": additions,\n",
    "        \"deletions\": deletions,\n",
    "        \"modifications\": modifications,\n",
    "        \"total_changes\": {\n",
    "            \"added_lines\": len(additions),\n",
    "            \"deleted_lines\": len(deletions),\n",
    "            \"modified_lines\": len(modifications),\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_diff_reports(parsed_packages: Dict[str, Dict[str, Dict[str, str]]]) -> Dict[str, Any]:\n",
    "    \"\"\"Generate diff reports comparing parsed packages from two databases.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f\"diff_reports_{timestamp}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    diff_reports = {}\n",
    "    db_names = list(parsed_packages.keys())\n",
    "\n",
    "    if len(db_names) < 2:\n",
    "        raise ValueError(\"Need at least two packages for comparison.\")\n",
    "\n",
    "    source_database, target_database = db_names[0], db_names[1]\n",
    "\n",
    "    global_missing_units = {\n",
    "        source_database: {\"missing_functions\": [], \"missing_procedures\": []},\n",
    "        target_database: {\"missing_functions\": [], \"missing_procedures\": []},\n",
    "    }\n",
    "\n",
    "    for package_name in parsed_packages[source_database].keys():\n",
    "        if package_name not in parsed_packages[target_database]:\n",
    "            global_missing_units[source_database][\"missing_procedures\"].append(package_name)\n",
    "            continue\n",
    "\n",
    "        package_diff_reports = {}\n",
    "        first_package_units = parsed_packages[source_database][package_name]\n",
    "        second_package_units = parsed_packages[target_database][package_name]\n",
    "\n",
    "        missing_in_first = set(second_package_units.keys()) - set(first_package_units.keys())\n",
    "        missing_in_second = set(first_package_units.keys()) - set(second_package_units.keys())\n",
    "\n",
    "        for missing in missing_in_first:\n",
    "            if missing.startswith(\"FUNCTION\"):\n",
    "                global_missing_units[target_database][\"missing_functions\"].append(missing)\n",
    "            elif missing.startswith(\"PROCEDURE\"):\n",
    "                global_missing_units[target_database][\"missing_procedures\"].append(missing)\n",
    "\n",
    "        for missing in missing_in_second:\n",
    "            if missing.startswith(\"FUNCTION\"):\n",
    "                global_missing_units[source_database][\"missing_functions\"].append(missing)\n",
    "            elif missing.startswith(\"PROCEDURE\"):\n",
    "                global_missing_units[source_database][\"missing_procedures\"].append(missing)\n",
    "\n",
    "        for unit_name in set(first_package_units.keys()) & set(second_package_units.keys()):\n",
    "            first_unit_code = first_package_units[unit_name]\n",
    "            second_unit_code = second_package_units[unit_name]\n",
    "\n",
    "            differences = compare_code(first_unit_code, second_unit_code)\n",
    "\n",
    "            differences[\"source_database\"] = source_database\n",
    "            differences[\"target_database\"] = target_database\n",
    "\n",
    "            if differences[\"total_changes\"][\"added_lines\"] > 0 or differences[\"total_changes\"][\"deleted_lines\"] > 0:\n",
    "                package_diff_reports[unit_name] = differences\n",
    "\n",
    "        if package_diff_reports:\n",
    "            diff_reports[package_name] = package_diff_reports\n",
    "\n",
    "    diff_reports[\"_global_missing_units\"] = global_missing_units\n",
    "\n",
    "    serializable_reports = ensure_json_serializable(diff_reports)\n",
    "\n",
    "    report_path = os.path.join(output_dir, \"code_differences_report.json\")\n",
    "    with open(report_path, \"w\") as report_file:\n",
    "        json.dump(serializable_reports, report_file, indent=2)\n",
    "\n",
    "    print(f\"Report saved at: {report_path}\")\n",
    "    return serializable_reports\n",
    "\n",
    "\n",
    "diff_reports = generate_diff_reports(parsed_packages)\n",
    "\n",
    "try:\n",
    "    json_str = json.dumps(diff_reports, indent=2)\n",
    "    print(\"The diff_reports object is valid JSON.\")\n",
    "except TypeError as e:\n",
    "    print(f\"JSON serialization error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c11d7b9-6726-49c3-ad15-3b861fc56a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report saved at: diff_reports_20241203_152601\\code_differences_report.json\n",
      "The diff_reports object is ready for JSON serialization.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List, Union\n",
    "import google.generativeai as genai\n",
    "\n",
    "GEMINI_API_KEY = 'AIzaSyCx4mBnY-2HWHa2KpVvsKUvIkUgxcp7jX0'\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "def ensure_json_serializable(obj):\n",
    "    \"\"\"Ensure the object is JSON-serializable.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: ensure_json_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [ensure_json_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    elif hasattr(obj, \"__dict__\"):\n",
    "        return ensure_json_serializable(obj.__dict__)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def parse_gemini_response(response_text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Robust parsing of Gemini response, handling various potential formats\n",
    "    \"\"\"\n",
    "    # Remove code block markers if present\n",
    "    response_text = response_text.strip('`')\n",
    "    \n",
    "    # Try parsing as JSON\n",
    "    try:\n",
    "        parsed_response = json.loads(response_text)\n",
    "        return parsed_response\n",
    "    except json.JSONDecodeError:\n",
    "        # If JSON parsing fails, try to extract JSON-like content\n",
    "        try:\n",
    "            # Find the first occurrence of a JSON-like structure\n",
    "            import re\n",
    "            json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                parsed_response = json.loads(json_match.group(0))\n",
    "                return parsed_response\n",
    "        except Exception as e:\n",
    "            print(f\"Could not parse Gemini response: {e}\")\n",
    "            print(f\"Raw response: {response_text}\")\n",
    "    \n",
    "    # Fallback default structure if parsing fails\n",
    "    return {\n",
    "        \"missing_in_source\": [],\n",
    "        \"missing_in_target\": [],\n",
    "        \"code_differences\": []\n",
    "    }\n",
    "\n",
    "def compare_packages_with_gemini(source_package: Dict[str, str], target_package: Dict[str, str], \n",
    "                                  source_db_name: str, target_db_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Enhanced package comparison using Gemini API\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Strictly compare two software packages and provide a structured JSON response.\n",
    "    Requirements:\n",
    "    - Respond ONLY in a valid JSON format\n",
    "    - Include these exact keys: 'missing_in_source', 'missing_in_target', 'code_differences'\n",
    "    - 'missing_in_source' and 'missing_in_target' should be lists of unit names\n",
    "    - 'code_differences' should list matching units with presence status\n",
    "\n",
    "    Source Package Units ({source_db_name}):\n",
    "    {json.dumps(list(source_package.keys()), indent=2)}\n",
    "\n",
    "    Target Package Units ({target_db_name}):\n",
    "    {json.dumps(list(target_package.keys()), indent=2)}\n",
    "\n",
    "    JSON Response Format:\n",
    "    {{\n",
    "      \"missing_in_source\": [\"PROCEDURE example1\", \"FUNCTION example2\"],\n",
    "      \"missing_in_target\": [\"PROCEDURE example3\", \"FUNCTION example4\"],\n",
    "      \"code_differences\": [\n",
    "        {{\n",
    "          \"unit_name\": \"PROCEDURE example\",\n",
    "          \"source\": \"Takes in 3 parameters\",\n",
    "          \"target\": \"Takes in 2 paramaters\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        \n",
    "        comparison_result = parse_gemini_response(response.text)\n",
    "        \n",
    "        package_diff_report = {\n",
    "            \"missing_units\": {\n",
    "                source_db_name: {\n",
    "                    \"missing_functions\": [\n",
    "                        unit for unit in comparison_result.get('missing_in_source', []) \n",
    "                        if unit.startswith(\"FUNCTION\")\n",
    "                    ],\n",
    "                    \"missing_procedures\": [\n",
    "                        unit for unit in comparison_result.get('missing_in_source', []) \n",
    "                        if unit.startswith(\"PROCEDURE\")\n",
    "                    ]\n",
    "                },\n",
    "                target_db_name: {\n",
    "                    \"missing_functions\": [\n",
    "                        unit for unit in comparison_result.get('missing_in_target', []) \n",
    "                        if unit.startswith(\"FUNCTION\")\n",
    "                    ],\n",
    "                    \"missing_procedures\": [\n",
    "                        unit for unit in comparison_result.get('missing_in_target', []) \n",
    "                        if unit.startswith(\"PROCEDURE\")\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"code_differences\": comparison_result.get('code_differences', {})\n",
    "        }\n",
    "        \n",
    "        return package_diff_report\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Gemini API call: {e}\")\n",
    "        return {\n",
    "            \"missing_units\": {\n",
    "                source_db_name: {\"missing_functions\": [], \"missing_procedures\": []},\n",
    "                target_db_name: {\"missing_functions\": [], \"missing_procedures\": []}\n",
    "            },\n",
    "            \"code_differences\": {}\n",
    "        }\n",
    "\n",
    "def generate_diff_reports(parsed_packages: Dict[str, Dict[str, Dict[str, str]]]) -> Dict[str, Any]:\n",
    "    \"\"\"Generate diff reports comparing parsed packages from two databases.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f\"diff_reports_{timestamp}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    diff_reports = {}\n",
    "    db_names = list(parsed_packages.keys())\n",
    "\n",
    "    if len(db_names) < 2:\n",
    "        raise ValueError(\"Need at least two databases for comparison.\")\n",
    "\n",
    "    source_database, target_database = db_names[0], db_names[1]\n",
    "\n",
    "    restructured_report = {\n",
    "        \"missing_units\": {\n",
    "            source_database: {\"missing_procedures\": [], \"missing_functions\": []},\n",
    "            target_database: {\"missing_procedures\": [], \"missing_functions\": []}\n",
    "        },\n",
    "        \"code_differences\": {}\n",
    "    }\n",
    "\n",
    "    for package_name in parsed_packages[source_database].keys():\n",
    "        if package_name not in parsed_packages[target_database]:\n",
    "            restructured_report[\"missing_units\"][source_database][\"missing_procedures\"].append(package_name)\n",
    "            continue\n",
    "\n",
    "        # Compare packages using Gemini\n",
    "        package_comparison = compare_packages_with_gemini(\n",
    "            parsed_packages[source_database][package_name],\n",
    "            parsed_packages[target_database][package_name],\n",
    "            source_database,\n",
    "            target_database\n",
    "        )\n",
    "\n",
    "        restructured_report[\"missing_units\"][source_database][\"missing_functions\"].extend(\n",
    "            package_comparison[\"missing_units\"][source_database][\"missing_functions\"]\n",
    "        )\n",
    "        restructured_report[\"missing_units\"][source_database][\"missing_procedures\"].extend(\n",
    "            package_comparison[\"missing_units\"][source_database][\"missing_procedures\"]\n",
    "        )\n",
    "        restructured_report[\"missing_units\"][target_database][\"missing_functions\"].extend(\n",
    "            package_comparison[\"missing_units\"][target_database][\"missing_functions\"]\n",
    "        )\n",
    "        restructured_report[\"missing_units\"][target_database][\"missing_procedures\"].extend(\n",
    "            package_comparison[\"missing_units\"][target_database][\"missing_procedures\"]\n",
    "        )\n",
    "\n",
    "        if package_comparison[\"code_differences\"]:\n",
    "            restructured_report[\"code_differences\"][package_name] = package_comparison[\"code_differences\"]\n",
    "\n",
    "    serializable_reports = ensure_json_serializable(restructured_report)\n",
    "\n",
    "    report_path = os.path.join(output_dir, \"code_differences_report.json\")\n",
    "    with open(report_path, \"w\") as report_file:\n",
    "        json.dump(serializable_reports, report_file, indent=2)\n",
    "\n",
    "    print(f\"Report saved at: {report_path}\")\n",
    "    return serializable_reports\n",
    "\n",
    "diff_reports = generate_diff_reports(parsed_packages)\n",
    "\n",
    "# try:\n",
    "#     json_str = json.dumps(diff_reports, indent=2)\n",
    "#     print(\"The diff_reports object is ready for JSON serialization.\")\n",
    "# except TypeError as e:\n",
    "#     print(f\"JSON serialization error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
