{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environment-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Prompt for API key\n",
    "#LLAMA_CLOUD_API_KEY = getpass(\"Enter your LLAMA CLOUD API key: \")\n",
    "#ANTHROPIC_API_KEY = getpass(\"Enter your Antropic API key: \")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# Install required packages\n",
    "# !pip install oracledb openai google-generativeai\n",
    "\n",
    "# Ensure Oracle Instant Client is installed\n",
    "# Refer to Oracle's documentation for installation instructions specific to your OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5b58a8-80d8-4721-93a9-8b48ab0af83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oracledb as cx_Oracle\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import difflib\n",
    "#import openai  # Import the OpenAI module\n",
    "# Import the Python SDK\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-modules",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Retrieve OpenAI API key from environment variable\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "if not GOOGLE_API_KEY:\n",
    "    logging.error(\"Google Gemini API key not found. Please set the 'GOOGLE_API_KEY' environment variable.\")\n",
    "else:\n",
    "    GOOGLE_API_KEY = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c42db8-9830-4e1c-8c6b-d2584d7d4b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=GOOGLE_API_KEY,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "database-credentials",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database credentials\n",
    "database_credentials = {\n",
    "    \"HERITAGE\": {\n",
    "        \"host\": \"10.176.18.91\",\n",
    "        \"port\": 1522,\n",
    "        \"service_name\": \"HERITAGE19C\",\n",
    "        \"username\": \"TQ_GIS\",\n",
    "        \"password\": \"TQ_GIS\"\n",
    "    },\n",
    "    \"NEW_GEMINIA\": {\n",
    "        \"host\": \"10.176.18.110\",\n",
    "        \"port\": 1521,\n",
    "        \"service_name\": \"NEW_GEMINIA\",\n",
    "        \"username\": \"TQ_GIS\",\n",
    "        \"password\": \"TQ_GIS\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a46016e-0fbe-4e9c-b9cf-1d73834c073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New function to generate markdown reports using GPT-4\n",
    "def generate_markdown_report(component_type, component_name, diff_file_path, report_dir='reports'):\n",
    "    \"\"\"\n",
    "    Sends the diff to GPT-4 and generates a markdown report.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Generating markdown report for {component_type} '{component_name}'.\")\n",
    "    os.makedirs(report_dir, exist_ok=True)\n",
    "    \n",
    "    with open(diff_file_path, 'r', encoding='utf-8') as f:\n",
    "        diff_content = f.read()\n",
    "    \n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"\n",
    "You are an expert PL/SQL developer. Below is a unified diff of a {component_type[:-1].capitalize()} named '{component_name}' between two versions of a PL/SQL package. Analyze the changes and generate a detailed markdown report with the following sections:\n",
    "\n",
    "- **Summary of Key Changes:**\n",
    "    - *Reordering of Conditional Logic:*\n",
    "        - **HERITAGE Version:**\n",
    "            - [Description]\n",
    "        - **NEW_GEMINIA Version:**\n",
    "            - [Description]\n",
    "    - *Modification of WHERE Clauses:*\n",
    "        - **Removal and Addition of Conditions:**\n",
    "            - [Description]\n",
    "    - *Exception Handling Adjustments:*\n",
    "        - **HERITAGE Version:**\n",
    "            - [Description]\n",
    "        - **NEW_GEMINIA Version:**\n",
    "            - [Description]\n",
    "    - *Formatting and Indentation:*\n",
    "        - [Description]\n",
    "\n",
    "- **Implications of the Changes:**\n",
    "    - *Logic Alteration in Fee Determination:*\n",
    "        - **Priority Shift:**\n",
    "            - **HERITAGE:** [Description]\n",
    "            - **NEW_GEMINIA:** [Description]\n",
    "        - **Potential Outcome Difference:**\n",
    "            - [Description]\n",
    "    - *Business Rule Alignment:*\n",
    "        - [Description]\n",
    "    - *Impact on Clients:*\n",
    "        - [Description]\n",
    "\n",
    "- **Recommendations for Merging:**\n",
    "    - *Review Business Requirements:*\n",
    "        - **Confirm Intent:**\n",
    "            - [Description]\n",
    "    - *Consult Stakeholders:*\n",
    "        - [Description]\n",
    "    - *Test Thoroughly:*\n",
    "        - **Create Test Cases:**\n",
    "            - [Description]\n",
    "        - **Validate Outcomes:**\n",
    "            - [Description]\n",
    "    - *Merge Strategy:*\n",
    "        - **Conditional Merge:**\n",
    "            - [Description]\n",
    "        - **Maintain Backward Compatibility:**\n",
    "            - [Description]\n",
    "    - *Update Documentation:*\n",
    "        - [Description]\n",
    "    - *Code Quality Improvements:*\n",
    "        - **Consistent Exception Handling:**\n",
    "            - [Description]\n",
    "        - **Clean Up Code:**\n",
    "            - [Description]\n",
    "\n",
    "- **Potential Actions Based on Analysis:**\n",
    "    - **If the Change Aligns with Business Goals:**\n",
    "        - [Description]\n",
    "    - **If the Change Does Not Align:**\n",
    "        - [Description]\n",
    "    - **If Uncertain:**\n",
    "        - [Description]\n",
    "\n",
    "- **Additional Considerations:**\n",
    "    - *Database Integrity:*\n",
    "        - [Description]\n",
    "    - *Performance Impact:*\n",
    "        - [Description]\n",
    "    - *Error Messages:*\n",
    "        - [Description]\n",
    "\n",
    "- **Conclusion:**\n",
    "    - [Summary of the overall analysis and final thoughts.]\n",
    "\n",
    "Below is the unified diff:\n",
    "\n",
    "```diff\n",
    "{diff_content}\n",
    "```\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "                    model=\"gemini-1.5-flash\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are an expert PL/SQL developer.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0.3,\n",
    "                    #max_tokens=2000,\n",
    "                    n=1,\n",
    "                    #stop=None,\n",
    "        )\n",
    "        print(\"RESPONSE FROM LLMs\", response.choices[0].message.content)\n",
    "        report = response.choices[0].message.content\n",
    "        \n",
    "        # Save the report to a markdown file\n",
    "        report_file_path = os.path.join(report_dir, f\"{component_type}_{component_name}_report.md\")\n",
    "        with open(report_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        print(\"WROTE REPORT\", report)\n",
    "        logging.info(f\"Markdown report generated and saved to '{report_file_path}'.\")\n",
    "        return True  # Indicate success\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to generate markdown report for {component_type} '{component_name}': {e}\")\n",
    "        # Save failed component information\n",
    "        failed_reports.append((component_type, component_name, diff_file_path))\n",
    "        return False  # Indicate failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functions-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions Definitions\n",
    "\n",
    "def get_package_source(db_params, package_name, object_type='PACKAGE BODY'):\n",
    "    logging.info(f\"Connecting to database {db_params['service_name']} to retrieve {object_type} '{package_name}'.\")\n",
    "    try:\n",
    "        dsn_tns = cx_Oracle.makedsn(\n",
    "            db_params['host'],\n",
    "            db_params['port'],\n",
    "            service_name=db_params['service_name']\n",
    "        )\n",
    "        conn = cx_Oracle.connect(\n",
    "            user=db_params['username'],\n",
    "            password=db_params['password'],\n",
    "            dsn=dsn_tns\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        query = f\"\"\"\n",
    "        SELECT text\n",
    "        FROM all_source\n",
    "        WHERE name = '{package_name.upper()}'\n",
    "        AND type = '{object_type.upper()}'\n",
    "        ORDER BY line\n",
    "        \"\"\"\n",
    "        cursor.execute(query)\n",
    "        source_lines = [row[0] for row in cursor.fetchall()]\n",
    "        source = ''.join(source_lines)\n",
    "        logging.info(f\"Retrieved {len(source)} characters of source code from {db_params['service_name']}.\")\n",
    "    except cx_Oracle.DatabaseError as e:\n",
    "        logging.error(f\"Database connection failed: {e}\")\n",
    "        source = \"\"\n",
    "    finally:\n",
    "        try:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "    return source\n",
    "\n",
    "def parse_package_components(source_code):\n",
    "    logging.info(\"Parsing package components.\")\n",
    "    components = {\n",
    "        'procedures': {},\n",
    "        'functions': {},\n",
    "        'cursors': {},\n",
    "        'types': {},\n",
    "        'variables': {},\n",
    "    }\n",
    "\n",
    "    # Define patterns for procedures and functions\n",
    "    proc_pattern = re.compile(\n",
    "        r\"\"\"\n",
    "        PROCEDURE\\s+([\\w$]+)\\s*               # Match PROCEDURE and its name\n",
    "        \\(.*?\\)\\s*                           # Match parameter list (if any), non-greedy\n",
    "        (.*?)                                # Match the body lazily\n",
    "        (?=PROCEDURE|FUNCTION|\\Z)            # Stop at the next PROCEDURE/FUNCTION or end of file\n",
    "        \"\"\",\n",
    "        re.IGNORECASE | re.DOTALL | re.VERBOSE\n",
    "    )\n",
    "    \n",
    "    func_pattern = re.compile(\n",
    "        r\"\"\"\n",
    "        FUNCTION\\s+([\\w$]+)\\s*               # Match FUNCTION and its name\n",
    "        \\(.*?\\)\\s*                           # Match parameter list (if any), non-greedy\n",
    "        (.*?)                                # Match the body lazily\n",
    "        (?=PROCEDURE|FUNCTION|\\Z)            # Stop at the next PROCEDURE/FUNCTION or end of file\n",
    "        \"\"\",\n",
    "        re.IGNORECASE | re.DOTALL | re.VERBOSE\n",
    "    )\n",
    "\n",
    "    # Extract procedures\n",
    "    procedure_matches = list(proc_pattern.finditer(source_code))\n",
    "    logging.info(f\"Found {len(procedure_matches)} procedures.\")\n",
    "    for match in procedure_matches:\n",
    "        name = match.group(1)\n",
    "        definition = match.group(0)\n",
    "        components['procedures'][name] = definition.strip()\n",
    "        logging.debug(f\"Parsed procedure: {name}\")\n",
    "\n",
    "    # Extract functions\n",
    "    function_matches = list(func_pattern.finditer(source_code))\n",
    "    logging.info(f\"Found {len(function_matches)} functions.\")\n",
    "    for match in function_matches:\n",
    "        name = match.group(1)\n",
    "        definition = match.group(0)\n",
    "        components['functions'][name] = definition.strip()\n",
    "        logging.debug(f\"Parsed function: {name}\")\n",
    "\n",
    "    # Extract the declaration section between IS/AS and BEGIN\n",
    "    declaration_section_match = re.search(r'(IS|AS)\\s+(.*?)\\s+BEGIN', source_code, re.IGNORECASE | re.DOTALL)\n",
    "    if declaration_section_match:\n",
    "        declaration_section = declaration_section_match.group(2)\n",
    "\n",
    "        # Patterns for cursors, types, and variables\n",
    "        cursor_pattern = re.compile(\n",
    "            r'CURSOR\\s+([\\w$]+)\\s*(IS|AS)\\s+(.*?);',\n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "\n",
    "        type_pattern = re.compile(\n",
    "            r'TYPE\\s+([\\w$]+)\\s+(IS|AS)\\s+(.*?);',\n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "\n",
    "        variable_pattern = re.compile(\n",
    "            r'(\\w+)\\s+(CONSTANT\\s+)?[\\w%\\.]+(\\([\\d\\s,]*\\))?(\\s+NOT\\s+NULL)?\\s*(:=\\s*.*?|)\\s*;',\n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "\n",
    "        # Extract cursors\n",
    "        cursor_matches = list(cursor_pattern.finditer(declaration_section))\n",
    "        logging.info(f\"Found {len(cursor_matches)} cursors.\")\n",
    "        for match in cursor_matches:\n",
    "            name = match.group(1)\n",
    "            definition = match.group(0)\n",
    "            components['cursors'][name] = definition.strip()\n",
    "            logging.debug(f\"Parsed cursor: {name}\")\n",
    "\n",
    "        # Extract types\n",
    "        type_matches = list(type_pattern.finditer(declaration_section))\n",
    "        logging.info(f\"Found {len(type_matches)} types.\")\n",
    "        for match in type_matches:\n",
    "            name = match.group(1)\n",
    "            definition = match.group(0)\n",
    "            components['types'][name] = definition.strip()\n",
    "            logging.debug(f\"Parsed type: {name}\")\n",
    "\n",
    "        # Extract variables\n",
    "        variable_matches = list(variable_pattern.finditer(declaration_section))\n",
    "        logging.info(f\"Found {len(variable_matches)} variables.\")\n",
    "        for match in variable_matches:\n",
    "            name = match.group(1)\n",
    "            definition = match.group(0)\n",
    "            components['variables'][name] = definition.strip()\n",
    "            logging.debug(f\"Parsed variable: {name}\")\n",
    "\n",
    "    logging.info(\"Finished parsing package components.\")\n",
    "    return components\n",
    "\n",
    "def save_components_to_disk(components, package_name, base_directory='packages'):\n",
    "    logging.info(f\"Saving components of '{package_name}' to disk.\")\n",
    "    package_dir = os.path.join(base_directory, package_name)\n",
    "    os.makedirs(package_dir, exist_ok=True)\n",
    "\n",
    "    total_components = 0\n",
    "    for comp_type, comp_dict in components.items():\n",
    "        type_dir = os.path.join(package_dir, comp_type)\n",
    "        os.makedirs(type_dir, exist_ok=True)\n",
    "        for name, definition in comp_dict.items():\n",
    "            # Clean the name to be file-system friendly\n",
    "            safe_name = ''.join(c if c.isalnum() or c in ' _-' else '_' for c in name)\n",
    "            file_name = f\"{safe_name}.sql\"\n",
    "            file_path = os.path.join(type_dir, file_name)\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(definition)\n",
    "            total_components += 1\n",
    "    logging.info(f\"Saved {total_components} components of '{package_name}' to '{package_dir}'.\")\n",
    "\n",
    "def save_components_as_json(components, package_name, base_directory='packages'):\n",
    "    logging.info(f\"Saving components of '{package_name}' as JSON.\")\n",
    "    package_dir = os.path.join(base_directory, package_name)\n",
    "    os.makedirs(package_dir, exist_ok=True)\n",
    "\n",
    "    json_path = os.path.join(package_dir, f\"{package_name}_components.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(components, f, indent=4)\n",
    "    logging.info(f\"Components saved as JSON to '{json_path}'.\")\n",
    "\n",
    "def compare_components(components1, components2, package_name):\n",
    "    logging.info(\"Comparing components with detailed diffs.\")\n",
    "    differences = {}\n",
    "    diffs_output_dir = os.path.join('diffs', package_name)\n",
    "    os.makedirs(diffs_output_dir, exist_ok=True)\n",
    "\n",
    "    for comp_type in components1.keys():\n",
    "        set1 = set(components1[comp_type].keys())\n",
    "        set2 = set(components2[comp_type].keys())\n",
    "\n",
    "        added = set2 - set1\n",
    "        removed = set1 - set2\n",
    "        modified = set()\n",
    "\n",
    "        for common in set1 & set2:\n",
    "            content1 = components1[comp_type][common].strip().splitlines()\n",
    "            content2 = components2[comp_type][common].strip().splitlines()\n",
    "            if content1 != content2:\n",
    "                modified.add(common)\n",
    "                # Generate diff\n",
    "                diff = difflib.unified_diff(\n",
    "                    content1, content2,\n",
    "                    fromfile=f'{package_name}_HERITAGE_{comp_type}_{common}.sql',\n",
    "                    tofile=f'{package_name}_NEW_GEMINIA_{comp_type}_{common}.sql',\n",
    "                    lineterm=''\n",
    "                )\n",
    "                diff_output = '\\n'.join(diff)\n",
    "                # Save diff to file\n",
    "                diff_file_path = os.path.join(diffs_output_dir, f'{comp_type}_{common}_diff.txt')\n",
    "                with open(diff_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(diff_output)\n",
    "                logging.debug(f\"Diff for {comp_type} '{common}' saved to '{diff_file_path}'.\")\n",
    "        differences[comp_type] = {\n",
    "            'added': list(added),\n",
    "            'removed': list(removed),\n",
    "            'modified': list(modified)\n",
    "        }\n",
    "    logging.info(\"Finished comparing components with diffs.\")\n",
    "    return differences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_plsql_packages(package_name, log_level='INFO'):\n",
    "    global failed_reports  # Declare as global to modify within the function\n",
    "    failed_reports = []  # List to keep track of failed report generations\n",
    "\n",
    "    # Set logging level\n",
    "    numeric_level = getattr(logging, log_level.upper(), None)\n",
    "    if not isinstance(numeric_level, int):\n",
    "        logging.warning(f\"Invalid log level: {log_level}. Defaulting to INFO.\")\n",
    "        numeric_level = logging.INFO\n",
    "    logging.getLogger().setLevel(numeric_level)\n",
    "\n",
    "    logging.info(f\"Starting comparison for package '{package_name}'.\")\n",
    "\n",
    "    # Get the package body source code from both databases\n",
    "    logging.info(\"Retrieving package sources.\")\n",
    "    source_body_heritage = get_package_source(database_credentials['HERITAGE'], package_name, 'PACKAGE BODY')\n",
    "    source_body_new_geminia = get_package_source(database_credentials['NEW_GEMINIA'], package_name, 'PACKAGE BODY')\n",
    "\n",
    "    if not source_body_heritage:\n",
    "        logging.error(f\"Failed to retrieve PACKAGE BODY from HERITAGE for package '{package_name}'.\")\n",
    "        return\n",
    "    if not source_body_new_geminia:\n",
    "        logging.error(f\"Failed to retrieve PACKAGE BODY from NEW_GEMINIA for package '{package_name}'.\")\n",
    "        return\n",
    "\n",
    "    # Parse components from package body\n",
    "    logging.info(\"Parsing package components from HERITAGE.\")\n",
    "    components_body_heritage = parse_package_components(source_body_heritage)\n",
    "    logging.info(\"Parsing package components from NEW_GEMINIA.\")\n",
    "    components_body_new_geminia = parse_package_components(source_body_new_geminia)\n",
    "\n",
    "    # Save components to disk\n",
    "    logging.info(\"Saving components to disk.\")\n",
    "    save_components_to_disk(components_body_heritage, package_name + '_HERITAGE_BODY')\n",
    "    save_components_to_disk(components_body_new_geminia, package_name + '_NEW_GEMINIA_BODY')\n",
    "\n",
    "    # Optionally, save as JSON\n",
    "    logging.info(\"Saving components as JSON.\")\n",
    "    save_components_as_json(components_body_heritage, package_name + '_HERITAGE_BODY')\n",
    "    save_components_as_json(components_body_new_geminia, package_name + '_NEW_GEMINIA_BODY')\n",
    "\n",
    "    # Compare packages with detailed diffs\n",
    "    differences = compare_components(components_body_heritage, components_body_new_geminia, package_name)\n",
    "\n",
    "    # Save differences to a JSON file for later use\n",
    "    differences_file = os.path.join('diffs', package_name, 'differences.json')\n",
    "    with open(differences_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(differences, f, indent=4)\n",
    "    logging.info(f\"Differences saved to '{differences_file}'.\")\n",
    "\n",
    "    # Output differences with summaries and generate markdown reports\n",
    "    logging.info(\"Outputting differences with summaries and generating markdown reports.\")\n",
    "    for comp_type, diff in differences.items():\n",
    "        print(f\"\\nDifferences in {comp_type}:\")\n",
    "        if diff['added']:\n",
    "            print(f\"  Added in NEW_GEMINIA: {diff['added']}\")\n",
    "        if diff['removed']:\n",
    "            print(f\"  Removed from NEW_GEMINIA: {diff['removed']}\")\n",
    "        if diff['modified']:\n",
    "            print(f\"  Modified: {diff['modified']}\")\n",
    "            for name in diff['modified']:\n",
    "                diff_file_path = os.path.join('diffs', package_name, f'{comp_type}_{name}_diff.txt')\n",
    "                print(f\"    - Diff for {name} saved at: {diff_file_path}\")\n",
    "                # Generate markdown report using GPT-4\n",
    "                success = generate_markdown_report(comp_type, name, diff_file_path)\n",
    "                if not success:\n",
    "                    logging.error(f\"Report generation failed for {comp_type} '{name}'.\")\n",
    "\n",
    "    # Check if any report generations failed\n",
    "    if failed_reports:\n",
    "        logging.error(\"Some report generations failed. You can retry generating reports for these components using the 'retry_failed_reports' function.\")\n",
    "        # Save failed reports info to a JSON file\n",
    "        failed_reports_file = os.path.join('reports', 'failed_reports.json')\n",
    "        with open(failed_reports_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(failed_reports, f, indent=4)\n",
    "        logging.info(f\"Failed report details saved to '{failed_reports_file}'.\")\n",
    "\n",
    "    logging.info(f\"Finished comparison for package '{package_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retry-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_failed_reports():\n",
    "    \"\"\"\n",
    "    Retries generating markdown reports for components that previously failed.\n",
    "    \"\"\"\n",
    "    global failed_reports\n",
    "    if not failed_reports:\n",
    "        logging.info(\"No failed reports to retry.\")\n",
    "        return\n",
    "\n",
    "    logging.info(\"Retrying failed markdown report generations.\")\n",
    "    successful_retries = []\n",
    "    remaining_failures = []\n",
    "\n",
    "    for comp_type, name, diff_file_path in failed_reports:\n",
    "        success = generate_markdown_report(comp_type, name, diff_file_path)\n",
    "        if success:\n",
    "            successful_retries.append((comp_type, name))\n",
    "        else:\n",
    "            remaining_failures.append((comp_type, name, diff_file_path))\n",
    "\n",
    "    if successful_retries:\n",
    "        logging.info(f\"Successfully retried and generated reports for: {successful_retries}\")\n",
    "\n",
    "    if remaining_failures:\n",
    "        logging.error(f\"Still failed to generate reports for: {[(ct, n) for ct, n, _ in remaining_failures]}\")\n",
    "        # Update the failed_reports global variable\n",
    "        failed_reports = remaining_failures\n",
    "        # Update the failed reports JSON file\n",
    "        failed_reports_file = os.path.join('reports', 'failed_reports.json')\n",
    "        with open(failed_reports_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(failed_reports, f, indent=4)\n",
    "    else:\n",
    "        logging.info(\"All failed reports have been successfully generated.\")\n",
    "        failed_reports = []  # Clear the failed reports list\n",
    "        # Remove the failed reports JSON file\n",
    "        failed_reports_file = os.path.join('reports', 'failed_reports.json')\n",
    "        if os.path.exists(failed_reports_file):\n",
    "            os.remove(failed_reports_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc32e5f8-3dc5-4836-98b7-a34d69cdf716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'YOUR_PACKAGE_NAME' with the actual package name and set desired log level\n",
    "# compare_plsql_packages(package_name='GIN_STP_PKG', log_level='DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bbb4e4-bd5f-4635-9898-fba7014d9031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there were any failures in report generation, you can retry them:\n",
    "# retry_failed_reports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ff22d-e449-4fa0-935a-7d4b91f1ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.CRITICAL, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Retrieve OpenAI API key from environment variable\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "if not GOOGLE_API_KEY:\n",
    "    logging.error(\"Google Gemini API key not found. Please set the 'GOOGLE_API_KEY' environment variable.\")\n",
    "else:\n",
    "    GOOGLE_API_KEY = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7dc700-3275-4df5-9be9-e7c9fe65cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure the model using the API key from the previous cell\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Optional: Set generation configuration (modify as needed)\n",
    "generation_config = {\n",
    "    \"temperature\": 0.2,  # Controls randomness (1=more random, 0=less random)\n",
    "    \"top_p\": 0.95,    # Probability distribution weighting\n",
    "    \"max_output_tokens\": 8192, \n",
    "}\n",
    "\n",
    "# Optionally create a GenerativeModel instance with specific settings\n",
    "model = genai.GenerativeModel(\n",
    "    # model_name=\"gemini-2.0-flash-exp\",\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=generation_config,\n",
    "    system_instruction=\" \",\n",
    ")\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"Gemini model configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba7099c-ed3d-41e0-8486-135405c21841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import google.generativeai as genai\n",
    "\n",
    "def merge_database_package_procedures_with_history(\n",
    "    differences_file_path: str,\n",
    "    heritage_package_procedure: str,\n",
    "    geminia_package_procedure: str,\n",
    "    output_package_path: str,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Merges procedures using Gemini with history and completion checks using 'FINAL_MERGING_DONE'.\n",
    "\n",
    "    Args:\n",
    "        differences_file_path: Path to the file containing differences between the procedures.\n",
    "        heritage_package_procedure: Path to the heritage procedure file.\n",
    "        geminia_package_procedure: Path to the geminia procedure file.\n",
    "        output_package_path: Path to save the merged procedure.\n",
    "\n",
    "    Returns:\n",
    "        Path of the saved merged procedure.\n",
    "    \"\"\"\n",
    "\n",
    "    genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "    generation_config = {\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_output_tokens\": 8192,\n",
    "        \"response_mime_type\": \"text/plain\",\n",
    "    }\n",
    "\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash-exp\",\n",
    "        generation_config=generation_config,\n",
    "        system_instruction=\"You are a senior PL/SQL developer proficient in merging procedures and functions. Indicate completion with the word 'FINAL_MERGING_DONE' at a new line at the bottom of the last code.\",\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with open(differences_file_path, \"r\") as diff_file:\n",
    "            diff_content = diff_file.read()\n",
    "\n",
    "        with open(heritage_package_procedure, \"r\") as heritage_file:\n",
    "            heritage_content = heritage_file.read()\n",
    "\n",
    "        with open(geminia_package_procedure, \"r\") as geminia_file:\n",
    "            geminia_content = geminia_file.read()\n",
    "\n",
    "        base_prompt = f\"\"\"\n",
    "You are an expert PL/SQL developer. Your task is to merge two plsql procedures into a single harmonized procedure.\n",
    "The differences between the two procedures are stored in here (you will only use this for context, dont paste it directly):\n",
    "{diff_content}\n",
    "The first procedure is here: Heritage Procedure:\n",
    "{heritage_content}\n",
    "The second procedure that will be used for comparison and merging is here: Geminia Procedure:\n",
    "{geminia_content}\n",
    "When the final output is done, write a new line below the final code saying \"FINAL_MERGING_DONE\" at the end of your response.\n",
    "        \"\"\"\n",
    "\n",
    "        merged_chunks = []\n",
    "        history = [{\"role\": \"user\", \"parts\": [base_prompt]}]\n",
    "        chat_session = model.start_chat(history=history)\n",
    "\n",
    "        while True:\n",
    "            response = chat_session.send_message(\"continue\")  \n",
    "            chunk = response.text.strip()\n",
    "            merged_chunks.append(chunk)\n",
    "            history.append({\"role\": \"model\", \"parts\": [chunk]})\n",
    "\n",
    "            if \"FINAL_MERGING_DONE\" in chunk.upper():\n",
    "                merged_chunks[-1] = merged_chunks[-1].replace(\"FINAL_MERGING_DONE\", \"\").strip()\n",
    "                break\n",
    "            else:\n",
    "                history.append({\"role\": \"user\", \"parts\": [\"Continue.\"]})\n",
    "                chat_session = model.start_chat(history=history)\n",
    "\n",
    "        merged_content = \"\\n\\n\".join(merged_chunks)\n",
    "        output_dir = os.path.dirname(output_package_path) or \".\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        with open(output_package_path, \"w\") as output_file:\n",
    "            output_file.write(merged_content)\n",
    "\n",
    "        logging.info(f\"Merged package written to {output_package_path}\")\n",
    "        return output_package_path\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"LLM Error: {e}\")\n",
    "        raise RuntimeError(f\"Failed to merge using LLM: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc8a31-36d5-4598-82e1-33f71f90e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "def merge_all_database_procedures_and_functions(package_name, differences_file, heritage_path, geminia_path, output_dir):\n",
    "    \"\"\"\n",
    "    Iterates through procedures/functions in the differences JSON file and merges them dynamically.\n",
    "    \n",
    "    Args:\n",
    "        package_name (str): Name of the package being processed.\n",
    "        differences_file (str): Path to the JSON file containing differences for all components.\n",
    "        heritage_path (str): Path to the Heritage package directory.\n",
    "        geminia_path (str): Path to the Geminia package directory.\n",
    "        output_dir (str): Directory to save the merged files.\n",
    "        llm_client (object): An instance of an LLM client for processing the merge.\n",
    "\n",
    "    Returns:\n",
    "        list: List of paths to the merged files.\n",
    "    \"\"\"\n",
    "    # Load differences JSON file\n",
    "    if not os.path.exists(differences_file):\n",
    "        raise FileNotFoundError(f\"Differences file not found: {differences_file}\")\n",
    "\n",
    "    with open(differences_file, 'r') as file:\n",
    "        differences = json.load(file)\n",
    "\n",
    "    merged_files = []\n",
    "\n",
    "    for comp_type, diff_data in differences.items():\n",
    "        # Process only procedures and functions\n",
    "        if comp_type not in ['procedures', 'functions']:\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"Processing {comp_type}...\")\n",
    "        for name in diff_data.get('modified', []) + diff_data.get('added', []):\n",
    "            # Define paths for this procedure/function\n",
    "            diff_file_path = os.path.join('diffs', package_name, f\"{comp_type}_{name}_diff.txt\")\n",
    "            heritage_file_path = os.path.join(heritage_path, comp_type, f\"{name}.sql\")\n",
    "            geminia_file_path = os.path.join(geminia_path, comp_type, f\"{name}.sql\")\n",
    "            output_file_path = os.path.join(output_dir, f\"merged_{name}.sql\")\n",
    "\n",
    "            # Check if required files exist\n",
    "            if not os.path.exists(diff_file_path):\n",
    "                logging.warning(f\"Diff file not found: {diff_file_path}. Skipping {name}.\")\n",
    "                continue\n",
    "            if not os.path.exists(heritage_file_path):\n",
    "                logging.warning(f\"Heritage procedure/function file not found: {heritage_file_path}. Skipping {name}.\")\n",
    "                continue\n",
    "            if not os.path.exists(geminia_file_path):\n",
    "                logging.warning(f\"Geminia procedure/function file not found: {geminia_file_path}. Skipping {name}.\")\n",
    "                continue\n",
    "\n",
    "            # Merge the procedure/function\n",
    "            try:\n",
    "                merged_file = merge_database_package_procedures_with_history(\n",
    "                    differences_file_path=diff_file_path,\n",
    "                    heritage_package_procedure=heritage_file_path,\n",
    "                    geminia_package_procedure=geminia_file_path,\n",
    "                    output_package_path=output_file_path\n",
    "                )\n",
    "                merged_files.append(merged_file)\n",
    "                logging.info(f\"Merged file created: {merged_file}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to merge {comp_type} '{name}': {e}\")\n",
    "\n",
    "    return merged_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d91bf-61a1-4884-8b3f-c146846efa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "package_name = \"GIN_STP_PKG\"\n",
    "differences_file = os.path.join(\"diffs\", package_name, \"differences.json\")\n",
    "merged_files = merge_all_database_procedures_and_functions(\n",
    "    package_name=package_name,\n",
    "    differences_file=differences_file,\n",
    "    heritage_path=\"packages/GIN_STP_PKG_HERITAGE_BODY\",\n",
    "    geminia_path=\"packages/GIN_STP_PKG_NEW_GEMINIA_BODY\",\n",
    "    output_dir=\"output\"\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Merged files created at: {merged_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d88e2-1e4b-4240-8b61-7a3e857ff261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "\n",
    "def clean_sql_code_blocks(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes ```sql at the beginning and ``` at the end of SQL files.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the SQL file to clean\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the cleaned file (same as input)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        # Remove ```sql from the beginning (case insensitive)\n",
    "        content = re.sub(r'^```sql\\s*\\n?', '', content, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove ``` from the end\n",
    "        content = re.sub(r'\\n?\\s*```\\s*$', '', content)\n",
    "        \n",
    "        # Write back the cleaned content\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(content)\n",
    "        \n",
    "        logging.info(f\"Cleaned code blocks from: {file_path}\")\n",
    "        return file_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error cleaning file {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def clean_all_sql_files_in_directory(directory_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Cleans all SQL files in a directory and its subdirectories.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing SQL files\n",
    "        \n",
    "    Returns:\n",
    "        list: List of cleaned file paths\n",
    "    \"\"\"\n",
    "    cleaned_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.sql'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    clean_sql_code_blocks(file_path)\n",
    "                    cleaned_files.append(file_path)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed to clean {file_path}: {e}\")\n",
    "    \n",
    "    return cleaned_files\n",
    "\n",
    "def clean_merged_files(merged_files_list: list) -> list:\n",
    "    \"\"\"\n",
    "    Cleans a list of merged SQL files.\n",
    "    \n",
    "    Args:\n",
    "        merged_files_list (list): List of file paths to clean\n",
    "        \n",
    "    Returns:\n",
    "        list: List of cleaned file paths\n",
    "    \"\"\"\n",
    "    cleaned_files = []\n",
    "    \n",
    "    for file_path in merged_files_list:\n",
    "        try:\n",
    "            clean_sql_code_blocks(file_path)\n",
    "            cleaned_files.append(file_path)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to clean {file_path}: {e}\")\n",
    "    \n",
    "    return cleaned_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c77711-5814-46a5-8c13-7f8d00bb7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def create_package_header(package_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates a standard package header with timestamp.\n",
    "    \n",
    "    Args:\n",
    "        package_name (str): Name of the package\n",
    "        \n",
    "    Returns:\n",
    "        str: Package header content\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    header = f\"\"\"-- =====================================================\n",
    "-- Package: {package_name}\n",
    "-- Generated: {timestamp}\n",
    "-- Description: Merged procedures and functions from heritage and geminia versions\n",
    "-- =====================================================\n",
    "\n",
    "CREATE OR REPLACE PACKAGE BODY {package_name} AS\n",
    "\n",
    "\"\"\"\n",
    "    return header\n",
    "\n",
    "def create_package_footer() -> str:\n",
    "    \"\"\"\n",
    "    Creates a standard package footer.\n",
    "    \n",
    "    Returns:\n",
    "        str: Package footer content\n",
    "    \"\"\"\n",
    "    return \"\\nEND;\\n/\"\n",
    "\n",
    "def clean_sql_content(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans SQL content by removing code blocks and extra whitespace.\n",
    "    \n",
    "    Args:\n",
    "        content (str): Raw SQL content\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned SQL content\n",
    "    \"\"\"\n",
    "    # Remove ```sql at the beginning (case insensitive)\n",
    "    content = re.sub(r'^```sql\\s*\\n?', '', content, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove ``` at the end\n",
    "    content = re.sub(r'\\n?\\s*```\\s*$', '', content)\n",
    "    \n",
    "    # Remove FINAL_MERGING_DONE if present\n",
    "    content = re.sub(r'\\n?\\s*FINAL_MERGING_DONE\\s*$', '', content, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Clean up extra whitespace but preserve SQL formatting\n",
    "    content = content.strip()\n",
    "    \n",
    "    return content\n",
    "\n",
    "def consolidate_merged_procedures_and_functions(\n",
    "    package_name: str,\n",
    "    merged_files_dir: str,\n",
    "    output_dir: str = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Consolidates all merged procedures and functions into a single package file.\n",
    "    \n",
    "    Args:\n",
    "        package_name (str): Name of the original package (e.g., \"GIN_STP_PKG\")\n",
    "        merged_files_dir (str): Directory containing merged SQL files\n",
    "        output_dir (str, optional): Directory to save the consolidated package. \n",
    "                                   Defaults to same as merged_files_dir\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the consolidated package file\n",
    "    \"\"\"\n",
    "    if output_dir is None:\n",
    "        output_dir = merged_files_dir\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate the consolidated package filename\n",
    "    consolidated_filename = f\"{package_name}_MERGED.sql\"\n",
    "    consolidated_path = os.path.join(output_dir, consolidated_filename)\n",
    "    \n",
    "    # Get all merged SQL files\n",
    "    merged_files = []\n",
    "    if os.path.exists(merged_files_dir):\n",
    "        for file in os.listdir(merged_files_dir):\n",
    "            if file.endswith('.sql') and file.startswith('merged_'):\n",
    "                merged_files.append(os.path.join(merged_files_dir, file))\n",
    "    \n",
    "    if not merged_files:\n",
    "        raise FileNotFoundError(f\"No merged files found in {merged_files_dir}\")\n",
    "    \n",
    "    # Sort files for consistent ordering\n",
    "    merged_files.sort()\n",
    "    \n",
    "    # Start building the consolidated content\n",
    "    consolidated_content = create_package_header(package_name)\n",
    "    \n",
    "    procedure_count = 0\n",
    "    function_count = 0\n",
    "    \n",
    "    # Process each merged file\n",
    "    for file_path in merged_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "            \n",
    "            # Clean the content\n",
    "            cleaned_content = clean_sql_content(content)\n",
    "            \n",
    "            if cleaned_content.strip():\n",
    "                # Extract the procedure/function name from filename\n",
    "                filename = os.path.basename(file_path)\n",
    "                component_name = filename.replace('merged_', '').replace('.sql', '')\n",
    "                \n",
    "                # Add a comment header for each component\n",
    "                consolidated_content += f\"\\n  -- {'-' * 50}\\n\"\n",
    "                consolidated_content += f\"  -- {component_name.upper()}\\n\"\n",
    "                consolidated_content += f\"  -- {'-' * 50}\\n\\n\"\n",
    "                \n",
    "                # Add the cleaned content with proper indentation\n",
    "                indented_content = '\\n'.join(['  ' + line if line.strip() else line \n",
    "                                            for line in cleaned_content.split('\\n')])\n",
    "                consolidated_content += indented_content + \"\\n\\n\"\n",
    "                \n",
    "                # Count procedures and functions\n",
    "                if 'PROCEDURE' in cleaned_content.upper():\n",
    "                    procedure_count += 1\n",
    "                elif 'FUNCTION' in cleaned_content.upper():\n",
    "                    function_count += 1\n",
    "                \n",
    "                logging.info(f\"Added {component_name} to consolidated package\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing file {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Add the package footer\n",
    "    consolidated_content += create_package_footer()\n",
    "    \n",
    "    # Write the consolidated package\n",
    "    with open(consolidated_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write(consolidated_content)\n",
    "    \n",
    "    logging.info(f\"Consolidated package created: {consolidated_path}\")\n",
    "    logging.info(f\"Total components: {procedure_count} procedures, {function_count} functions\")\n",
    "    \n",
    "    return consolidated_path\n",
    "\n",
    "def consolidate_from_file_list(\n",
    "    package_name: str,\n",
    "    merged_files_list: list,\n",
    "    output_dir: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Consolidates procedures and functions from a list of file paths.\n",
    "    \n",
    "    Args:\n",
    "        package_name (str): Name of the original package\n",
    "        merged_files_list (list): List of paths to merged SQL files\n",
    "        output_dir (str): Directory to save the consolidated package\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the consolidated package file\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate the consolidated package filename\n",
    "    consolidated_filename = f\"{package_name}_MERGED.sql\"\n",
    "    consolidated_path = os.path.join(output_dir, consolidated_filename)\n",
    "    \n",
    "    if not merged_files_list:\n",
    "        raise ValueError(\"No merged files provided\")\n",
    "    \n",
    "    # Start building the consolidated content\n",
    "    consolidated_content = create_package_header(package_name)\n",
    "    \n",
    "    procedure_count = 0\n",
    "    function_count = 0\n",
    "    \n",
    "    # Process each merged file\n",
    "    for file_path in sorted(merged_files_list):\n",
    "        if not os.path.exists(file_path):\n",
    "            logging.warning(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "            \n",
    "            # Clean the content\n",
    "            cleaned_content = clean_sql_content(content)\n",
    "            \n",
    "            if cleaned_content.strip():\n",
    "                # Extract the procedure/function name from filename\n",
    "                filename = os.path.basename(file_path)\n",
    "                component_name = filename.replace('merged_', '').replace('.sql', '')\n",
    "                \n",
    "                # Add a comment header for each component\n",
    "                consolidated_content += f\"\\n  -- {'-' * 50}\\n\"\n",
    "                consolidated_content += f\"  -- {component_name.upper()}\\n\"\n",
    "                consolidated_content += f\"  -- {'-' * 50}\\n\\n\"\n",
    "                \n",
    "                # Add the cleaned content with proper indentation\n",
    "                indented_content = '\\n'.join(['  ' + line if line.strip() else line \n",
    "                                            for line in cleaned_content.split('\\n')])\n",
    "                consolidated_content += indented_content + \"\\n\\n\"\n",
    "                \n",
    "                # Count procedures and functions\n",
    "                if 'PROCEDURE' in cleaned_content.upper():\n",
    "                    procedure_count += 1\n",
    "                elif 'FUNCTION' in cleaned_content.upper():\n",
    "                    function_count += 1\n",
    "                \n",
    "                logging.info(f\"Added {component_name} to consolidated package\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing file {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Add the package footer\n",
    "    consolidated_content += create_package_footer()\n",
    "    \n",
    "    # Write the consolidated package\n",
    "    with open(consolidated_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write(consolidated_content)\n",
    "    \n",
    "    logging.info(f\"Consolidated package created: {consolidated_path}\")\n",
    "    logging.info(f\"Total components: {procedure_count} procedures, {function_count} functions\")\n",
    "    \n",
    "    return consolidated_path\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c4fe5f-9270-4999-94d4-939e82a242cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Consolidate from directory\n",
    "package_name = \"GIN_STP_PKG\"\n",
    "consolidated_package = consolidate_merged_procedures_and_functions(\n",
    "\tpackage_name=package_name,\n",
    "\tmerged_files_dir=\"output\",\n",
    "\toutput_dir=\"consolidated_packages\"\n",
    ")\n",
    "print(f\"Consolidated package created: {consolidated_package}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd77b70d-0af1-45e5-9a08-5d0490a78c42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
