{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "environment-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cx_Oracle\n",
      "  Using cached cx_Oracle-8.3.0-cp313-cp313-win_amd64.whl\n",
      "Collecting openai\n",
      "  Downloading openai-1.64.0-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jackwere\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.6.2.post1)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jackwere\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (0.28.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.8.2-cp313-cp313-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jackwere\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (1.3.1)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jackwere\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\jackwere\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jackwere\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jackwere\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.27.2-cp313-cp313-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\jackwere\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.64.0-py3-none-any.whl (472 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.8.2-cp313-cp313-win_amd64.whl (203 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.8/2.0 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.0/2.0 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/2.0 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.6/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 1.1 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: cx_Oracle, typing-extensions, tqdm, jiter, distro, annotated-types, pydantic-core, pydantic, openai\n",
      "Successfully installed annotated-types-0.7.0 cx_Oracle-8.3.0 distro-1.9.0 jiter-0.8.2 openai-1.64.0 pydantic-2.10.6 pydantic-core-2.27.2 tqdm-4.67.1 typing-extensions-4.12.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Prompt for API key\n",
    "#LLAMA_CLOUD_API_KEY = getpass(\"Enter your LLAMA CLOUD API key: \")\n",
    "#ANTHROPIC_API_KEY = getpass(\"Enter your Antropic API key: \")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# Install required packages\n",
    "!pip install cx_Oracle openai\n",
    "\n",
    "# Ensure Oracle Instant Client is installed\n",
    "# Refer to Oracle's documentation for installation instructions specific to your OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b5b58a8-80d8-4721-93a9-8b48ab0af83f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cx_Oracle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcx_Oracle\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cx_Oracle'"
     ]
    }
   ],
   "source": [
    "import cx_Oracle\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import difflib\n",
    "#import openai  # Import the OpenAI module\n",
    "# Import the Python SDK\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "import-modules",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Retrieve OpenAI API key from environment variable\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "if not GOOGLE_API_KEY:\n",
    "    logging.error(\"Google Gemini API key not found. Please set the 'GOOGLE_API_KEY' environment variable.\")\n",
    "else:\n",
    "    GOOGLE_API_KEY = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7c42db8-9830-4e1c-8c6b-d2584d7d4b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=GOOGLE_API_KEY,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "database-credentials",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database credentials\n",
    "database_credentials = {\n",
    "    \"HERITAGE\": {\n",
    "        \"host\": \"10.176.18.91\",\n",
    "        \"port\": 1522,\n",
    "        \"service_name\": \"HERITAGE19C\",\n",
    "        \"username\": \"TQ_GIS\",\n",
    "        \"password\": \"TQ_GIS\"\n",
    "    },\n",
    "    \"NEW_GEMINIA\": {\n",
    "        \"host\": \"10.176.18.110\",\n",
    "        \"port\": 1523,\n",
    "        \"service_name\": \"NEW_GEMINIA\",\n",
    "        \"username\": \"TQ_GIS\",\n",
    "        \"password\": \"TQ_GIS\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a46016e-0fbe-4e9c-b9cf-1d73834c073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New function to generate markdown reports using GPT-4\n",
    "def generate_markdown_report(component_type, component_name, diff_file_path, report_dir='reports'):\n",
    "    \"\"\"\n",
    "    Sends the diff to GPT-4 and generates a markdown report.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Generating markdown report for {component_type} '{component_name}'.\")\n",
    "    os.makedirs(report_dir, exist_ok=True)\n",
    "    \n",
    "    with open(diff_file_path, 'r', encoding='utf-8') as f:\n",
    "        diff_content = f.read()\n",
    "    \n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"\n",
    "You are an expert PL/SQL developer. Below is a unified diff of a {component_type[:-1].capitalize()} named '{component_name}' between two versions of a PL/SQL package. Analyze the changes and generate a detailed markdown report with the following sections:\n",
    "\n",
    "- **Summary of Key Changes:**\n",
    "    - *Reordering of Conditional Logic:*\n",
    "        - **HERITAGE Version:**\n",
    "            - [Description]\n",
    "        - **NEW_GEMINIA Version:**\n",
    "            - [Description]\n",
    "    - *Modification of WHERE Clauses:*\n",
    "        - **Removal and Addition of Conditions:**\n",
    "            - [Description]\n",
    "    - *Exception Handling Adjustments:*\n",
    "        - **HERITAGE Version:**\n",
    "            - [Description]\n",
    "        - **NEW_GEMINIA Version:**\n",
    "            - [Description]\n",
    "    - *Formatting and Indentation:*\n",
    "        - [Description]\n",
    "\n",
    "- **Implications of the Changes:**\n",
    "    - *Logic Alteration in Fee Determination:*\n",
    "        - **Priority Shift:**\n",
    "            - **HERITAGE:** [Description]\n",
    "            - **NEW_GEMINIA:** [Description]\n",
    "        - **Potential Outcome Difference:**\n",
    "            - [Description]\n",
    "    - *Business Rule Alignment:*\n",
    "        - [Description]\n",
    "    - *Impact on Clients:*\n",
    "        - [Description]\n",
    "\n",
    "- **Recommendations for Merging:**\n",
    "    - *Review Business Requirements:*\n",
    "        - **Confirm Intent:**\n",
    "            - [Description]\n",
    "    - *Consult Stakeholders:*\n",
    "        - [Description]\n",
    "    - *Test Thoroughly:*\n",
    "        - **Create Test Cases:**\n",
    "            - [Description]\n",
    "        - **Validate Outcomes:**\n",
    "            - [Description]\n",
    "    - *Merge Strategy:*\n",
    "        - **Conditional Merge:**\n",
    "            - [Description]\n",
    "        - **Maintain Backward Compatibility:**\n",
    "            - [Description]\n",
    "    - *Update Documentation:*\n",
    "        - [Description]\n",
    "    - *Code Quality Improvements:*\n",
    "        - **Consistent Exception Handling:**\n",
    "            - [Description]\n",
    "        - **Clean Up Code:**\n",
    "            - [Description]\n",
    "\n",
    "- **Potential Actions Based on Analysis:**\n",
    "    - **If the Change Aligns with Business Goals:**\n",
    "        - [Description]\n",
    "    - **If the Change Does Not Align:**\n",
    "        - [Description]\n",
    "    - **If Uncertain:**\n",
    "        - [Description]\n",
    "\n",
    "- **Additional Considerations:**\n",
    "    - *Database Integrity:*\n",
    "        - [Description]\n",
    "    - *Performance Impact:*\n",
    "        - [Description]\n",
    "    - *Error Messages:*\n",
    "        - [Description]\n",
    "\n",
    "- **Conclusion:**\n",
    "    - [Summary of the overall analysis and final thoughts.]\n",
    "\n",
    "Below is the unified diff:\n",
    "\n",
    "```diff\n",
    "{diff_content}\n",
    "```\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "                    model=\"gemini-1.5-flash\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are an expert PL/SQL developer.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0.3,\n",
    "                    #max_tokens=2000,\n",
    "                    n=1,\n",
    "                    #stop=None,\n",
    "        )\n",
    "        print(\"RESPONSE FROM LLMs\", response.choices[0].message.content)\n",
    "        report = response.choices[0].message.content\n",
    "        \n",
    "        # Save the report to a markdown file\n",
    "        report_file_path = os.path.join(report_dir, f\"{component_type}_{component_name}_report.md\")\n",
    "        with open(report_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        print(\"WROTE REPORT\", report)\n",
    "        logging.info(f\"Markdown report generated and saved to '{report_file_path}'.\")\n",
    "        return True  # Indicate success\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to generate markdown report for {component_type} '{component_name}': {e}\")\n",
    "        # Save failed component information\n",
    "        failed_reports.append((component_type, component_name, diff_file_path))\n",
    "        return False  # Indicate failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "functions-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions Definitions\n",
    "\n",
    "def get_package_source(db_params, package_name, object_type='PACKAGE BODY'):\n",
    "    logging.info(f\"Connecting to database {db_params['service_name']} to retrieve {object_type} '{package_name}'.\")\n",
    "    try:\n",
    "        dsn_tns = cx_Oracle.makedsn(\n",
    "            db_params['host'],\n",
    "            db_params['port'],\n",
    "            service_name=db_params['service_name']\n",
    "        )\n",
    "        conn = cx_Oracle.connect(\n",
    "            user=db_params['username'],\n",
    "            password=db_params['password'],\n",
    "            dsn=dsn_tns\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        query = f\"\"\"\n",
    "        SELECT text\n",
    "        FROM all_source\n",
    "        WHERE name = '{package_name.upper()}'\n",
    "        AND type = '{object_type.upper()}'\n",
    "        ORDER BY line\n",
    "        \"\"\"\n",
    "        cursor.execute(query)\n",
    "        source_lines = [row[0] for row in cursor.fetchall()]\n",
    "        source = ''.join(source_lines)\n",
    "        logging.info(f\"Retrieved {len(source)} characters of source code from {db_params['service_name']}.\")\n",
    "    except cx_Oracle.DatabaseError as e:\n",
    "        logging.error(f\"Database connection failed: {e}\")\n",
    "        source = \"\"\n",
    "    finally:\n",
    "        try:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "    return source\n",
    "\n",
    "def parse_package_components(source_code):\n",
    "    logging.info(\"Parsing package components.\")\n",
    "    components = {\n",
    "        'procedures': {},\n",
    "        'functions': {},\n",
    "        'cursors': {},\n",
    "        'types': {},\n",
    "        'variables': {},\n",
    "    }\n",
    "\n",
    "    # Define patterns for procedures and functions\n",
    "    proc_pattern = re.compile(\n",
    "        r\"\"\"\n",
    "        PROCEDURE\\s+([\\w$]+)\\s*               # Match PROCEDURE and its name\n",
    "        \\(.*?\\)\\s*                           # Match parameter list (if any), non-greedy\n",
    "        (.*?)                                # Match the body lazily\n",
    "        (?=PROCEDURE|FUNCTION|\\Z)            # Stop at the next PROCEDURE/FUNCTION or end of file\n",
    "        \"\"\",\n",
    "        re.IGNORECASE | re.DOTALL | re.VERBOSE\n",
    "    )\n",
    "    \n",
    "    func_pattern = re.compile(\n",
    "        r\"\"\"\n",
    "        FUNCTION\\s+([\\w$]+)\\s*               # Match FUNCTION and its name\n",
    "        \\(.*?\\)\\s*                           # Match parameter list (if any), non-greedy\n",
    "        (.*?)                                # Match the body lazily\n",
    "        (?=PROCEDURE|FUNCTION|\\Z)            # Stop at the next PROCEDURE/FUNCTION or end of file\n",
    "        \"\"\",\n",
    "        re.IGNORECASE | re.DOTALL | re.VERBOSE\n",
    "    )\n",
    "\n",
    "    # Extract procedures\n",
    "    procedure_matches = list(proc_pattern.finditer(source_code))\n",
    "    logging.info(f\"Found {len(procedure_matches)} procedures.\")\n",
    "    for match in procedure_matches:\n",
    "        name = match.group(1)\n",
    "        definition = match.group(0)\n",
    "        components['procedures'][name] = definition.strip()\n",
    "        logging.debug(f\"Parsed procedure: {name}\")\n",
    "\n",
    "    # Extract functions\n",
    "    function_matches = list(func_pattern.finditer(source_code))\n",
    "    logging.info(f\"Found {len(function_matches)} functions.\")\n",
    "    for match in function_matches:\n",
    "        name = match.group(1)\n",
    "        definition = match.group(0)\n",
    "        components['functions'][name] = definition.strip()\n",
    "        logging.debug(f\"Parsed function: {name}\")\n",
    "\n",
    "    # Extract the declaration section between IS/AS and BEGIN\n",
    "    declaration_section_match = re.search(r'(IS|AS)\\s+(.*?)\\s+BEGIN', source_code, re.IGNORECASE | re.DOTALL)\n",
    "    if declaration_section_match:\n",
    "        declaration_section = declaration_section_match.group(2)\n",
    "\n",
    "        # Patterns for cursors, types, and variables\n",
    "        cursor_pattern = re.compile(\n",
    "            r'CURSOR\\s+([\\w$]+)\\s*(IS|AS)\\s+(.*?);',\n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "\n",
    "        type_pattern = re.compile(\n",
    "            r'TYPE\\s+([\\w$]+)\\s+(IS|AS)\\s+(.*?);',\n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "\n",
    "        variable_pattern = re.compile(\n",
    "            r'(\\w+)\\s+(CONSTANT\\s+)?[\\w%\\.]+(\\([\\d\\s,]*\\))?(\\s+NOT\\s+NULL)?\\s*(:=\\s*.*?|)\\s*;',\n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "\n",
    "        # Extract cursors\n",
    "        cursor_matches = list(cursor_pattern.finditer(declaration_section))\n",
    "        logging.info(f\"Found {len(cursor_matches)} cursors.\")\n",
    "        for match in cursor_matches:\n",
    "            name = match.group(1)\n",
    "            definition = match.group(0)\n",
    "            components['cursors'][name] = definition.strip()\n",
    "            logging.debug(f\"Parsed cursor: {name}\")\n",
    "\n",
    "        # Extract types\n",
    "        type_matches = list(type_pattern.finditer(declaration_section))\n",
    "        logging.info(f\"Found {len(type_matches)} types.\")\n",
    "        for match in type_matches:\n",
    "            name = match.group(1)\n",
    "            definition = match.group(0)\n",
    "            components['types'][name] = definition.strip()\n",
    "            logging.debug(f\"Parsed type: {name}\")\n",
    "\n",
    "        # Extract variables\n",
    "        variable_matches = list(variable_pattern.finditer(declaration_section))\n",
    "        logging.info(f\"Found {len(variable_matches)} variables.\")\n",
    "        for match in variable_matches:\n",
    "            name = match.group(1)\n",
    "            definition = match.group(0)\n",
    "            components['variables'][name] = definition.strip()\n",
    "            logging.debug(f\"Parsed variable: {name}\")\n",
    "\n",
    "    logging.info(\"Finished parsing package components.\")\n",
    "    return components\n",
    "\n",
    "def save_components_to_disk(components, package_name, base_directory='packages'):\n",
    "    logging.info(f\"Saving components of '{package_name}' to disk.\")\n",
    "    package_dir = os.path.join(base_directory, package_name)\n",
    "    os.makedirs(package_dir, exist_ok=True)\n",
    "\n",
    "    total_components = 0\n",
    "    for comp_type, comp_dict in components.items():\n",
    "        type_dir = os.path.join(package_dir, comp_type)\n",
    "        os.makedirs(type_dir, exist_ok=True)\n",
    "        for name, definition in comp_dict.items():\n",
    "            # Clean the name to be file-system friendly\n",
    "            safe_name = ''.join(c if c.isalnum() or c in ' _-' else '_' for c in name)\n",
    "            file_name = f\"{safe_name}.sql\"\n",
    "            file_path = os.path.join(type_dir, file_name)\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(definition)\n",
    "            total_components += 1\n",
    "    logging.info(f\"Saved {total_components} components of '{package_name}' to '{package_dir}'.\")\n",
    "\n",
    "def save_components_as_json(components, package_name, base_directory='packages'):\n",
    "    logging.info(f\"Saving components of '{package_name}' as JSON.\")\n",
    "    package_dir = os.path.join(base_directory, package_name)\n",
    "    os.makedirs(package_dir, exist_ok=True)\n",
    "\n",
    "    json_path = os.path.join(package_dir, f\"{package_name}_components.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(components, f, indent=4)\n",
    "    logging.info(f\"Components saved as JSON to '{json_path}'.\")\n",
    "\n",
    "def compare_components(components1, components2, package_name):\n",
    "    logging.info(\"Comparing components with detailed diffs.\")\n",
    "    differences = {}\n",
    "    diffs_output_dir = os.path.join('diffs', package_name)\n",
    "    os.makedirs(diffs_output_dir, exist_ok=True)\n",
    "\n",
    "    for comp_type in components1.keys():\n",
    "        set1 = set(components1[comp_type].keys())\n",
    "        set2 = set(components2[comp_type].keys())\n",
    "\n",
    "        added = set2 - set1\n",
    "        removed = set1 - set2\n",
    "        modified = set()\n",
    "\n",
    "        for common in set1 & set2:\n",
    "            content1 = components1[comp_type][common].strip().splitlines()\n",
    "            content2 = components2[comp_type][common].strip().splitlines()\n",
    "            if content1 != content2:\n",
    "                modified.add(common)\n",
    "                # Generate diff\n",
    "                diff = difflib.unified_diff(\n",
    "                    content1, content2,\n",
    "                    fromfile=f'{package_name}_HERITAGE_{comp_type}_{common}.sql',\n",
    "                    tofile=f'{package_name}_NEW_GEMINIA_{comp_type}_{common}.sql',\n",
    "                    lineterm=''\n",
    "                )\n",
    "                diff_output = '\\n'.join(diff)\n",
    "                # Save diff to file\n",
    "                diff_file_path = os.path.join(diffs_output_dir, f'{comp_type}_{common}_diff.txt')\n",
    "                with open(diff_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(diff_output)\n",
    "                logging.debug(f\"Diff for {comp_type} '{common}' saved to '{diff_file_path}'.\")\n",
    "        differences[comp_type] = {\n",
    "            'added': list(added),\n",
    "            'removed': list(removed),\n",
    "            'modified': list(modified)\n",
    "        }\n",
    "    logging.info(\"Finished comparing components with diffs.\")\n",
    "    return differences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "main-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_plsql_packages(package_name, log_level='INFO'):\n",
    "    global failed_reports  # Declare as global to modify within the function\n",
    "    failed_reports = []  # List to keep track of failed report generations\n",
    "\n",
    "    # Set logging level\n",
    "    numeric_level = getattr(logging, log_level.upper(), None)\n",
    "    if not isinstance(numeric_level, int):\n",
    "        logging.warning(f\"Invalid log level: {log_level}. Defaulting to INFO.\")\n",
    "        numeric_level = logging.INFO\n",
    "    logging.getLogger().setLevel(numeric_level)\n",
    "\n",
    "    logging.info(f\"Starting comparison for package '{package_name}'.\")\n",
    "\n",
    "    # Get the package body source code from both databases\n",
    "    logging.info(\"Retrieving package sources.\")\n",
    "    source_body_heritage = get_package_source(database_credentials['HERITAGE'], package_name, 'PACKAGE BODY')\n",
    "    source_body_new_geminia = get_package_source(database_credentials['NEW_GEMINIA'], package_name, 'PACKAGE BODY')\n",
    "\n",
    "    if not source_body_heritage:\n",
    "        logging.error(f\"Failed to retrieve PACKAGE BODY from HERITAGE for package '{package_name}'.\")\n",
    "        return\n",
    "    if not source_body_new_geminia:\n",
    "        logging.error(f\"Failed to retrieve PACKAGE BODY from NEW_GEMINIA for package '{package_name}'.\")\n",
    "        return\n",
    "\n",
    "    # Parse components from package body\n",
    "    logging.info(\"Parsing package components from HERITAGE.\")\n",
    "    components_body_heritage = parse_package_components(source_body_heritage)\n",
    "    logging.info(\"Parsing package components from NEW_GEMINIA.\")\n",
    "    components_body_new_geminia = parse_package_components(source_body_new_geminia)\n",
    "\n",
    "    # Save components to disk\n",
    "    logging.info(\"Saving components to disk.\")\n",
    "    save_components_to_disk(components_body_heritage, package_name + '_HERITAGE_BODY')\n",
    "    save_components_to_disk(components_body_new_geminia, package_name + '_NEW_GEMINIA_BODY')\n",
    "\n",
    "    # Optionally, save as JSON\n",
    "    logging.info(\"Saving components as JSON.\")\n",
    "    save_components_as_json(components_body_heritage, package_name + '_HERITAGE_BODY')\n",
    "    save_components_as_json(components_body_new_geminia, package_name + '_NEW_GEMINIA_BODY')\n",
    "\n",
    "    # Compare packages with detailed diffs\n",
    "    differences = compare_components(components_body_heritage, components_body_new_geminia, package_name)\n",
    "\n",
    "    # Save differences to a JSON file for later use\n",
    "    differences_file = os.path.join('diffs', package_name, 'differences.json')\n",
    "    with open(differences_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(differences, f, indent=4)\n",
    "    logging.info(f\"Differences saved to '{differences_file}'.\")\n",
    "\n",
    "    # Output differences with summaries and generate markdown reports\n",
    "    logging.info(\"Outputting differences with summaries and generating markdown reports.\")\n",
    "    for comp_type, diff in differences.items():\n",
    "        print(f\"\\nDifferences in {comp_type}:\")\n",
    "        if diff['added']:\n",
    "            print(f\"  Added in NEW_GEMINIA: {diff['added']}\")\n",
    "        if diff['removed']:\n",
    "            print(f\"  Removed from NEW_GEMINIA: {diff['removed']}\")\n",
    "        if diff['modified']:\n",
    "            print(f\"  Modified: {diff['modified']}\")\n",
    "            for name in diff['modified']:\n",
    "                diff_file_path = os.path.join('diffs', package_name, f'{comp_type}_{name}_diff.txt')\n",
    "                print(f\"    - Diff for {name} saved at: {diff_file_path}\")\n",
    "                # Generate markdown report using GPT-4\n",
    "                success = generate_markdown_report(comp_type, name, diff_file_path)\n",
    "                if not success:\n",
    "                    logging.error(f\"Report generation failed for {comp_type} '{name}'.\")\n",
    "\n",
    "    # Check if any report generations failed\n",
    "    if failed_reports:\n",
    "        logging.error(\"Some report generations failed. You can retry generating reports for these components using the 'retry_failed_reports' function.\")\n",
    "        # Save failed reports info to a JSON file\n",
    "        failed_reports_file = os.path.join('reports', 'failed_reports.json')\n",
    "        with open(failed_reports_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(failed_reports, f, indent=4)\n",
    "        logging.info(f\"Failed report details saved to '{failed_reports_file}'.\")\n",
    "\n",
    "    logging.info(f\"Finished comparison for package '{package_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "retry-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_failed_reports():\n",
    "    \"\"\"\n",
    "    Retries generating markdown reports for components that previously failed.\n",
    "    \"\"\"\n",
    "    global failed_reports\n",
    "    if not failed_reports:\n",
    "        logging.info(\"No failed reports to retry.\")\n",
    "        return\n",
    "\n",
    "    logging.info(\"Retrying failed markdown report generations.\")\n",
    "    successful_retries = []\n",
    "    remaining_failures = []\n",
    "\n",
    "    for comp_type, name, diff_file_path in failed_reports:\n",
    "        success = generate_markdown_report(comp_type, name, diff_file_path)\n",
    "        if success:\n",
    "            successful_retries.append((comp_type, name))\n",
    "        else:\n",
    "            remaining_failures.append((comp_type, name, diff_file_path))\n",
    "\n",
    "    if successful_retries:\n",
    "        logging.info(f\"Successfully retried and generated reports for: {successful_retries}\")\n",
    "\n",
    "    if remaining_failures:\n",
    "        logging.error(f\"Still failed to generate reports for: {[(ct, n) for ct, n, _ in remaining_failures]}\")\n",
    "        # Update the failed_reports global variable\n",
    "        failed_reports = remaining_failures\n",
    "        # Update the failed reports JSON file\n",
    "        failed_reports_file = os.path.join('reports', 'failed_reports.json')\n",
    "        with open(failed_reports_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(failed_reports, f, indent=4)\n",
    "    else:\n",
    "        logging.info(\"All failed reports have been successfully generated.\")\n",
    "        failed_reports = []  # Clear the failed reports list\n",
    "        # Remove the failed reports JSON file\n",
    "        failed_reports_file = os.path.join('reports', 'failed_reports.json')\n",
    "        if os.path.exists(failed_reports_file):\n",
    "            os.remove(failed_reports_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc32e5f8-3dc5-4836-98b7-a34d69cdf716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'YOUR_PACKAGE_NAME' with the actual package name and set desired log level\n",
    "# compare_plsql_packages(package_name='GIN_STP_PKG', log_level='DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8bbb4e4-bd5f-4635-9898-fba7014d9031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 14:32:19,181 - INFO - No failed reports to retry.\n"
     ]
    }
   ],
   "source": [
    "# If there were any failures in report generation, you can retry them:\n",
    "# retry_failed_reports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e46ff22d-e449-4fa0-935a-7d4b91f1ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Retrieve OpenAI API key from environment variable\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "if not GOOGLE_API_KEY:\n",
    "    logging.error(\"Google Gemini API key not found. Please set the 'GOOGLE_API_KEY' environment variable.\")\n",
    "else:\n",
    "    GOOGLE_API_KEY = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d7dc700-3275-4df5-9be9-e7c9fe65cb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini model configured successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure the model using the API key from the previous cell\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Optional: Set generation configuration (modify as needed)\n",
    "generation_config = {\n",
    "    \"temperature\": 0.2,  # Controls randomness (1=more random, 0=less random)\n",
    "    \"top_p\": 0.95,    # Probability distribution weighting\n",
    "    \"max_output_tokens\": 8192, \n",
    "}\n",
    "\n",
    "# Optionally create a GenerativeModel instance with specific settings\n",
    "model = genai.GenerativeModel(\n",
    "    # model_name=\"gemini-2.0-flash-exp\",\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=generation_config,\n",
    "    system_instruction=\" \",\n",
    ")\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"Gemini model configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c398d77a-5e21-4699-b25d-cd8ce05d78ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merged_file = merge_database_packages(\n",
    "#     report_path=\"reports/procedures_gin_policies_prc_report.md\",\n",
    "#     heritage_package_procedure=\"packages/GIN_STP_PKG_HERITAGE_BODY/procedures/gin_policies_prc.sql\",\n",
    "#     geminia_package_procedure=\"packages/GIN_STP_PKG_NEW_GEMINIA_BODY/procedures/gin_policies_prc.sql\",\n",
    "#     output_package_path=\"output/merged_package_procedure.sql\",\n",
    "#     llm_client=client\n",
    "# )\n",
    "# print(f\"Merged package created at: {merged_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b5d8d00-70b0-44f4-91d2-84f94401e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import logging\n",
    "\n",
    "\n",
    "# def merge_database_package_procedures_with_history(differences_file_path, heritage_package_procedure, geminia_package_procedure, output_package_path, llm_client):\n",
    "#     \"\"\"\n",
    "#     Merges two database package procedures (Heritage and Geminia) into a harmonized procedure\n",
    "#     using an LLM based on the differences file generated by difflib.\n",
    "\n",
    "#     Args:\n",
    "#         differences_file_path (str): Path to the differences file generated by difflib.\n",
    "#         heritage_package_procedure (str): Path to the Heritage database package.\n",
    "#         geminia_package_procedure (str): Path to the Geminia database package.\n",
    "#         output_package_path (str): Path where the merged procedures will be saved as .sql files.\n",
    "#         llm_client (object): An instance of an LLM client for processing the merge.\n",
    "\n",
    "#     Returns:\n",
    "#         str: Path to the merged package procedure file.\n",
    "#     \"\"\"\n",
    "#     # Step 1: Load the differences file\n",
    "#     with open(differences_file_path, 'r') as diff_file:\n",
    "#         diff_content = diff_file.read()\n",
    "\n",
    "#     # Step 2: Load the contents of the database packages\n",
    "#     with open(heritage_package_procedure, 'r') as heritage_file:\n",
    "#         heritage_content = heritage_file.read()\n",
    "\n",
    "#     with open(geminia_package_procedure, 'r') as geminia_file:\n",
    "#         geminia_content = geminia_file.read()\n",
    "\n",
    "#     # Step 3: Construct the initial prompt for the LLM\n",
    "#     base_prompt = f\"\"\"\n",
    "# You are an expert PL/SQL developer. Your task is to merge two plsql procedures into a single harmonized procedure.\n",
    "\n",
    "# Below are the differences report between the two procedures, generated using a detailed comparison tool:\n",
    "# {diff_content}\n",
    "\n",
    "# Here is the source code of the Heritage package procedure:\n",
    "# {heritage_content}\n",
    "\n",
    "# Here is the source code of the Geminia package procedure:\n",
    "# {geminia_content}\n",
    "\n",
    "# Generate a merged package procedure that incorporates changes from both sources, resolving any conflicts based on the provided differences. If the merged procedure exceeds the allowed token limit, provide the output in chunks, and indicate where to continue in each chunk until the last chunk when it is complete.\n",
    "# \"\"\"\n",
    "\n",
    "#     # Step 4: Generate the merged procedures iteratively\n",
    "#     merged_chunks = []\n",
    "#     continuation_token = \"Continue from here:\"\n",
    "#     chat_history = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are a senior PL/SQL developer. Generate the code in chunks if necessary and provide clear continuation points.\"},\n",
    "#         {\"role\": \"user\", \"content\": base_prompt}\n",
    "#     ]\n",
    "\n",
    "#     try:\n",
    "#         while True:\n",
    "#             # Send the current chat history to the model\n",
    "#             response = llm_client.chat.completions.create(\n",
    "#                 model=\"gemini-1.5-flash\",\n",
    "#                 messages=chat_history,\n",
    "#                 n=1\n",
    "#             )\n",
    "#             chunk = response.choices[0].message.content\n",
    "\n",
    "#             # Append the generated chunk to the merged_chunks list\n",
    "#             if continuation_token in chunk:\n",
    "#                 chunk, continuation_marker = chunk.split(continuation_token, 1)\n",
    "#                 merged_chunks.append(chunk.strip())\n",
    "\n",
    "#                 # Update chat history for continuation\n",
    "#                 chat_history.append({\"role\": \"assistant\", \"content\": chunk.strip()})\n",
    "#                 chat_history.append({\"role\": \"user\", \"content\": continuation_token.strip()})\n",
    "#             else:\n",
    "#                 merged_chunks.append(chunk.strip())\n",
    "#                 break\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Failed to generate merged package using LLM: {e}\")\n",
    "#         raise RuntimeError(f\"Failed to generate merged package using LLM: {e}\")\n",
    "\n",
    "#     # Step 5: Combine all chunks and write to the output file\n",
    "#     merged_content = \"\\n\\n\".join(merged_chunks)\n",
    "#     output_dir = os.path.dirname(output_package_path) or \".\"\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     with open(output_package_path, 'w') as output_file:\n",
    "#         output_file.write(merged_content)\n",
    "\n",
    "#     logging.info(f\"Merged package written to {output_package_path}\")\n",
    "#     return output_package_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aba7099c-ed3d-41e0-8486-135405c21841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import google.generativeai as genai\n",
    "\n",
    "def merge_database_package_procedures_with_history(\n",
    "    differences_file_path: str,\n",
    "    heritage_package_procedure: str,\n",
    "    geminia_package_procedure: str,\n",
    "    output_package_path: str,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Merges procedures using Gemini with history and completion checks using 'FINAL_MERGING_DONE'.\n",
    "\n",
    "    Args:\n",
    "        differences_file_path: Path to the file containing differences between the procedures.\n",
    "        heritage_package_procedure: Path to the heritage procedure file.\n",
    "        geminia_package_procedure: Path to the geminia procedure file.\n",
    "        output_package_path: Path to save the merged procedure.\n",
    "\n",
    "    Returns:\n",
    "        Path of the saved merged procedure.\n",
    "    \"\"\"\n",
    "\n",
    "    genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "    generation_config = {\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_output_tokens\": 8192,\n",
    "        \"response_mime_type\": \"text/plain\",\n",
    "    }\n",
    "\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash-exp\",\n",
    "        generation_config=generation_config,\n",
    "        system_instruction=\"You are a senior PL/SQL developer proficient in merging procedures and functions. Indicate completion with the word 'FINAL_MERGING_DONE' at a new line at the bottom of the last code.\",\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with open(differences_file_path, \"r\") as diff_file:\n",
    "            diff_content = diff_file.read()\n",
    "\n",
    "        with open(heritage_package_procedure, \"r\") as heritage_file:\n",
    "            heritage_content = heritage_file.read()\n",
    "\n",
    "        with open(geminia_package_procedure, \"r\") as geminia_file:\n",
    "            geminia_content = geminia_file.read()\n",
    "\n",
    "        base_prompt = f\"\"\"\n",
    "You are an expert PL/SQL developer. Your task is to merge two plsql procedures into a single harmonized procedure.\n",
    "The differences between the two procedures are stored in here (you will only use this for context, dont paste it directly):\n",
    "{diff_content}\n",
    "The first procedure is here: Heritage Procedure:\n",
    "{heritage_content}\n",
    "The second procedure that will be used for comparison and merging is here: Geminia Procedure:\n",
    "{geminia_content}\n",
    "When the final output is done, write a new line below the final code saying \"FINAL_MERGING_DONE\" at the end of your response.\n",
    "        \"\"\"\n",
    "\n",
    "        merged_chunks = []\n",
    "        history = [{\"role\": \"user\", \"parts\": [base_prompt]}]\n",
    "        chat_session = model.start_chat(history=history)\n",
    "\n",
    "        while True:\n",
    "            response = chat_session.send_message(\"continue\")  \n",
    "            chunk = response.text.strip()\n",
    "            merged_chunks.append(chunk)\n",
    "            history.append({\"role\": \"model\", \"parts\": [chunk]})\n",
    "\n",
    "            if \"FINAL_MERGING_DONE\" in chunk.upper():\n",
    "                merged_chunks[-1] = merged_chunks[-1].replace(\"FINAL_MERGING_DONE\", \"\").strip()\n",
    "                break\n",
    "            else:\n",
    "                history.append({\"role\": \"user\", \"parts\": [\"Continue.\"]})\n",
    "                chat_session = model.start_chat(history=history)\n",
    "\n",
    "        merged_content = \"\\n\\n\".join(merged_chunks)\n",
    "        output_dir = os.path.dirname(output_package_path) or \".\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        with open(output_package_path, \"w\") as output_file:\n",
    "            output_file.write(merged_content)\n",
    "\n",
    "        logging.info(f\"Merged package written to {output_package_path}\")\n",
    "        return output_package_path\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"LLM Error: {e}\")\n",
    "        raise RuntimeError(f\"Failed to merge using LLM: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69bc8a31-36d5-4598-82e1-33f71f90e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "def merge_all_database_procedures_and_functions(package_name, differences_file, heritage_path, geminia_path, output_dir):\n",
    "    \"\"\"\n",
    "    Iterates through procedures/functions in the differences JSON file and merges them dynamically.\n",
    "    \n",
    "    Args:\n",
    "        package_name (str): Name of the package being processed.\n",
    "        differences_file (str): Path to the JSON file containing differences for all components.\n",
    "        heritage_path (str): Path to the Heritage package directory.\n",
    "        geminia_path (str): Path to the Geminia package directory.\n",
    "        output_dir (str): Directory to save the merged files.\n",
    "        llm_client (object): An instance of an LLM client for processing the merge.\n",
    "\n",
    "    Returns:\n",
    "        list: List of paths to the merged files.\n",
    "    \"\"\"\n",
    "    # Load differences JSON file\n",
    "    if not os.path.exists(differences_file):\n",
    "        raise FileNotFoundError(f\"Differences file not found: {differences_file}\")\n",
    "\n",
    "    with open(differences_file, 'r') as file:\n",
    "        differences = json.load(file)\n",
    "\n",
    "    merged_files = []\n",
    "\n",
    "    for comp_type, diff_data in differences.items():\n",
    "        # Process only procedures and functions\n",
    "        if comp_type not in ['procedures', 'functions']:\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"Processing {comp_type}...\")\n",
    "        for name in diff_data.get('modified', []) + diff_data.get('added', []):\n",
    "            # Define paths for this procedure/function\n",
    "            diff_file_path = os.path.join('diffs', package_name, f\"{comp_type}_{name}_diff.txt\")\n",
    "            heritage_file_path = os.path.join(heritage_path, comp_type, f\"{name}.sql\")\n",
    "            geminia_file_path = os.path.join(geminia_path, comp_type, f\"{name}.sql\")\n",
    "            output_file_path = os.path.join(output_dir, f\"merged_{name}.sql\")\n",
    "\n",
    "            # Check if required files exist\n",
    "            if not os.path.exists(diff_file_path):\n",
    "                logging.warning(f\"Diff file not found: {diff_file_path}. Skipping {name}.\")\n",
    "                continue\n",
    "            if not os.path.exists(heritage_file_path):\n",
    "                logging.warning(f\"Heritage procedure/function file not found: {heritage_file_path}. Skipping {name}.\")\n",
    "                continue\n",
    "            if not os.path.exists(geminia_file_path):\n",
    "                logging.warning(f\"Geminia procedure/function file not found: {geminia_file_path}. Skipping {name}.\")\n",
    "                continue\n",
    "\n",
    "            # Merge the procedure/function\n",
    "            try:\n",
    "                merged_file = merge_database_package_procedures_with_history(\n",
    "                    differences_file_path=diff_file_path,\n",
    "                    heritage_package_procedure=heritage_file_path,\n",
    "                    geminia_package_procedure=geminia_file_path,\n",
    "                    output_package_path=output_file_path\n",
    "                )\n",
    "                merged_files.append(merged_file)\n",
    "                logging.info(f\"Merged file created: {merged_file}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to merge {comp_type} '{name}': {e}\")\n",
    "\n",
    "    return merged_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d91bf-61a1-4884-8b3f-c146846efa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "package_name = \"GIN_STP_PKG\"\n",
    "differences_file = os.path.join(\"diffs\", package_name, \"differences.json\")\n",
    "merged_files = merge_all_database_procedures_and_functions(\n",
    "    package_name=package_name,\n",
    "    differences_file=differences_file,\n",
    "    heritage_path=\"packages/GIN_STP_PKG_HERITAGE_BODY\",\n",
    "    geminia_path=\"packages/GIN_STP_PKG_NEW_GEMINIA_BODY\",\n",
    "    output_dir=\"output\"\n",
    ")\n",
    "\n",
    "# # Optional post-processing\n",
    "# for merged_file in merged_files:\n",
    "#     with open(merged_file, 'r') as file:\n",
    "#         content = file.read()\n",
    "#     if \"Continue from here:\" in content:\n",
    "#         content = content.replace(\"Continue from here:\", \"\").strip()\n",
    "#         with open(merged_file, 'w') as file:\n",
    "#             file.write(content)\n",
    "\n",
    "print(f\"Merged files created at: {merged_files}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
